{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-containerized-venv-edf_sentiment_analysis-cpu-m-1-cpu-4gb-ram",
      "display_name": "Python in CPU-M-1-cpu-4Gb-Ram (env edf_sentiment_analysis)",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.9.20",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "creator": "anne-soline.guilbert-ly@dataiku.com",
    "customFields": {},
    "createdOn": 1742400046150,
    "tags": [],
    "modifiedBy": "anne-soline.guilbert-ly@dataiku.com"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pylab inline"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nfrom dataiku import pandasutils as pdu\nimport pandas as pd"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example: load a DSS dataset as a Pandas dataframe\ncleaned_tweets \u003d dataiku.Dataset(\"cleaned_tweets\")\ncleaned_tweets_df \u003d cleaned_tweets.get_dataframe()\n\n# Dataset sensible_data_encrypted renamed to sensitive_data_encrypted by anne-soline.guilbert-ly@dataiku.com on 2025-03-19 20:29:19\n# Dataset sensitive_data_encrypted renamed to cleaned_tweets_encryption by anne-soline.guilbert-ly@dataiku.com on 2025-03-20 07:38:01\nsensible_data_encrypted \u003d dataiku.Dataset(\"cleaned_tweets_encryption\")\nsensible_data_encrypted_df \u003d sensible_data_encrypted.get_dataframe()\n\nsensible_data_removed_df \u003d dataiku.Dataset(\"sensible_data_removed_df\")\nsensible_data_removed_df \u003d sensible_data_removed_df.get_dataframe()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Remove \u0027text\u0027 column and rename \u0027encrypted_text\u0027 to \u0027text\u0027 for sensible_data_encrypted_df\nsensible_data_encrypted_df \u003d sensible_data_encrypted_df.drop(columns\u003d[\u0027text\u0027])\nsensible_data_encrypted_df \u003d sensible_data_encrypted_df.rename(columns\u003d{\u0027encrypted_text\u0027: \u0027text\u0027})\n\n# Remove \u0027text\u0027 column and rename \u0027encrypted_text\u0027 to \u0027text\u0027 for sensible_data_removed_df\nsensible_data_removed_df \u003d sensible_data_removed_df.drop(columns\u003d[\u0027text\u0027])\nsensible_data_removed_df \u003d sensible_data_removed_df.rename(columns\u003d{\u0027encrypted_text\u0027: \u0027text\u0027})\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.1. Retrait des duplicatas"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Remove duplicates based on the \u0027is_duplicated\u0027 column\ncleaned_tweets_df \u003d cleaned_tweets_df[cleaned_tweets_df[\u0027is_duplicated\u0027] \u003d\u003d 0]\nsensible_data_encrypted_df \u003d sensible_data_encrypted_df[sensible_data_encrypted_df[\u0027is_duplicated\u0027] \u003d\u003d 0]\nsensible_data_removed_df \u003d sensible_data_removed_df[sensible_data_removed_df[\u0027is_duplicated\u0027] \u003d\u003d 0]\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.2. Stemming"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Apply stemming to the \u0027text\u0027 column\nfrom nltk.stem import PorterStemmer\n\nstemmer \u003d PorterStemmer()\n\ndef apply_stemming(text):\n    words \u003d text.split()\n    stemmed_words \u003d [stemmer.stem(word) for word in words]\n    return \u0027 \u0027.join(stemmed_words)\n\ncleaned_tweets_df[\u0027text\u0027] \u003d cleaned_tweets_df[\u0027text\u0027].apply(apply_stemming)\nsensible_data_encrypted_df[\u0027text\u0027] \u003d sensible_data_encrypted_df[\u0027text\u0027].apply(apply_stemming)\nsensible_data_removed_df[\u0027text\u0027] \u003d sensible_data_removed_df[\u0027text\u0027].apply(apply_stemming)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cleaned_tweets_df.head()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sensible_data_encrypted_df.head()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sensible_data_removed_df.head()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    }
  ]
}