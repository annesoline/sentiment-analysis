{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python in GPU-S-1-GPU (env edf_sentiment_analysis)",
      "language": "python",
      "name": "py-dku-containerized-venv-edf_sentiment_analysis-gpu-s-1-gpu"
    },
    "associatedRecipe": "6_verification_sensitive_data",
    "creator": "anne-soline.guilbert-ly@dataiku.com",
    "createdOn": 1743411819304,
    "tags": [
      "recipe-editor"
    ],
    "customFields": {}
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# Load the required libraries\n",
        "import os\n",
        "import torch\n",
        "import dataiku\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from dataiku import pandasutils as pdu\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "pd.set_option(\u0027display.max_colwidth\u0027, None)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "labelled_tweets \u003d dataiku.Dataset(\"labelled_tweets\")\n",
        "df \u003d labelled_tweets.get_dataframe()\n",
        "\n",
        "# 6. Vérification de la présence de données sensibles\n",
        "## 6.1. Named Entities Recognition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "hf_transformers_home_dir \u003d os.getenv(\"HF_HOME\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# Compute recipe outputs from inputs\n",
        "# Define the model to use\n",
        "model_name \u003d \"dslim/bert-base-NER\"\n",
        "\n",
        "def perform_ner_inference(model_name, input_df):\n",
        "    \"\"\"\n",
        "    perform_ner_inference performs NER inference on a dataframe using a specified Hugging Face model.\n",
        "\n",
        "    :param model_name: The name of the Hugging Face model to use for NER.\n",
        "    :param input_df: The input dataframe with at least two columns, document_id and text.\n",
        "    :return: pd.DataFrame. A dataframe containing the NER results, with at least columns \"document_id\", \"text\", and \"predicted_labels\".\n",
        "    \"\"\"\n",
        "    # Load the pre-trained tokenizer and model\n",
        "    tokenizer \u003d AutoTokenizer.from_pretrained(model_name, cache_dir\u003dhf_transformers_home_dir)\n",
        "    model \u003d AutoModelForTokenClassification.from_pretrained(model_name, cache_dir\u003dhf_transformers_home_dir)\n",
        "\n",
        "    # Load the token classification pipeline\n",
        "    token_classification_pipeline \u003d pipeline(\"ner\", model\u003dmodel, tokenizer\u003dtokenizer, aggregation_strategy\u003d\"first\") # pass device\u003d0 if using gpu\n",
        "\n",
        "    # Perform token classification on each row of the dataframe\n",
        "    predicted_labels \u003d []\n",
        "    for index, row in input_df.iterrows():\n",
        "        document_id \u003d row[\"id\"]\n",
        "        text \u003d row[\"text\"]\n",
        "        results \u003d token_classification_pipeline(text)\n",
        "        predicted_labels.append(results)\n",
        "\n",
        "    input_df[\u0027NER\u0027] \u003d predicted_labels\n",
        "\n",
        "    return input_df"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "df \u003d perform_ner_inference(model_name, df)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "## 6.2. Reconnaissance et encryption ou retrait des emails et adresses IP\n",
        "import re\n",
        "\n",
        "# Define a function to extract email addresses\n",
        "def extract_email(text):\n",
        "    # Regular expression pattern for matching email addresses\n",
        "    email_pattern \u003d r\u0027[\\w\\-\\.]+@([\\w-]+\\.)+[\\w-]{2,}\u0027\n",
        "    # Search for the pattern in the text\n",
        "    match \u003d re.search(email_pattern, text)\n",
        "    if match:\n",
        "        return match.group(0)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Apply the function to the \u0027text\u0027 column and create a new column \u0027email_present\u0027\n",
        "df[\u0027email_present\u0027] \u003d df[\u0027text\u0027].apply(extract_email)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# Define a function to check for IP addresses and URLs containing IPs\n",
        "def check_ip_or_url_presence(text):\n",
        "    # Regular expression pattern for matching IP addresses\n",
        "    ip_pattern \u003d r\u0027\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b\u0027\n",
        "    # Regular expression pattern for matching URLs containing IP addresses\n",
        "    url_pattern \u003d r\u0027\\b(?:http|https)://(?:[0-9]{1,3}\\.){3}[0-9]{1,3}(?:/[^\\s]*)?\\b\u0027\n",
        "\n",
        "    # Search for the URL pattern first\n",
        "    url_match \u003d re.search(url_pattern, text)\n",
        "    if url_match:\n",
        "        return url_match.group(0)\n",
        "\n",
        "    # If no URL is found, search for the IP pattern\n",
        "    ip_match \u003d re.search(ip_pattern, text)\n",
        "    if ip_match:\n",
        "        return ip_match.group(0)\n",
        "\n",
        "    return None\n",
        "\n",
        "# Apply the function to the \u0027text\u0027 column and create a new column \u0027ip_present\u0027\n",
        "df[\u0027ip_present\u0027] \u003d df[\u0027text\u0027].apply(check_ip_or_url_presence)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# Recipe outputs\n",
        "sensitive_data_identified \u003d dataiku.Dataset(\"sensitive_data_identified\")\n",
        "sensitive_data_identified.write_with_schema(df)"
      ]
    }
  ]
}