{
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.9.20",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "py-dku-containerized-venv-edf_sentiment_analysis-cpu-xl-2-cpu-16gb-ram",
      "display_name": "Python in CPU-XL-2-cpu-16Gb-Ram (env edf_sentiment_analysis)",
      "language": "python"
    },
    "hide_input": false,
    "creator": "anne-soline.guilbert-ly@dataiku.com",
    "customFields": {},
    "createdOn": 1742456649095,
    "tags": [
      "deleted-recipe-editor"
    ],
    "modifiedBy": "anne-soline.guilbert-ly@dataiku.com"
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nimport pandas as pd\nimport re"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sensitive_data_identified \u003d dataiku.Dataset(\"sensitive_data_identified\")\ndf \u003d sensitive_data_identified.get_dataframe()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. Nettoyage de la donnée\n## 7.1. Encryption des données sensibles"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df[\"encrypted_text\"] \u003d df[\"text\"]\ndf[\"text_without_sensitive_data\"] \u003d df[\"text\"]"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encryption des NER"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from cryptography.fernet import Fernet\n\n# Generate a key for encryption\nkey \u003d Fernet.generate_key()\ncipher_suite \u003d Fernet(key)\n\n# Define a function to encrypt text\ndef encrypt_text(text):\n    return cipher_suite.encrypt(text.encode()).decode()\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def process_NER(row):\n    ner_list \u003d eval(row[\u0027NER\u0027])\n    if ner_list:\n        for entity in ner_list:\n            word \u003d entity[\u0027word\u0027]\n            # Encrypt and replace the word with its encrypted version in encrypted_text\n            row[\u0027encrypted_text\u0027] \u003d re.sub(r\u0027\\b\u0027 + re.escape(word) + r\u0027\\b\u0027, encrypt_text(word), row[\u0027encrypted_text\u0027])\n            # Remove the word from text_without_entities\n            row[\u0027text_without_sensitive_data\u0027] \u003d re.sub(r\u0027\\b\u0027 + re.escape(word) + r\u0027\\b\u0027, \u0027\u0027, row[\u0027text_without_sensitive_data\u0027])\n\n    return row\n\n# Apply the function to the dataframe\ndf \u003d df.apply(process_NER, axis\u003d1)\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encryption des emails et adresses IP"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Encrypt emails and IPs if present, otherwise fill with original text\ndef process_emails_and_ip(row):\n    if pd.notnull(row[\u0027email_present\u0027]):\n        # Encrypt email in encrypted_text column\n        row[\u0027encrypted_text\u0027] \u003d re.sub(row[\u0027email_present\u0027], \n                             lambda match: encrypt_text(match.group()), row[\u0027encrypted_text\u0027])\n\n        # Remove email in text_without_sensitive_data column\n        row[\u0027text_without_sensitive_data\u0027] \u003d row[\u0027text_without_sensitive_data\u0027].replace(row[\u0027email_present\u0027], \u0027\u0027)\n\n    if pd.notnull(row[\u0027ip_present\u0027]):\n        # Encrypt IP in encrypted_text column\n        row[\u0027encrypted_text\u0027] \u003d re.sub(row[\u0027ip_present\u0027], \n                             lambda match: encrypt_text(match.group()), row[\u0027encrypted_text\u0027])\n        \n        # Remove IP in text_without_sensitive_data column\n        row[\u0027encrypted_text\u0027] \u003d row[\u0027encrypted_text\u0027].replace(row[\u0027ip_present\u0027], \u0027\u0027)\n    \n    return row\n\n# Apply the encryption function to the dataframe\ndf \u003d df.apply(process_emails_and_ip, axis\u003d1)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Print the text of rows where email_present is not None and ip_present is not None\nemail_and_ip_present_rows \u003d df[df[\u0027email_present\u0027].notnull()| df[\u0027ip_present\u0027].notnull()]\nfor index, row in email_and_ip_present_rows.iterrows():\n    print(row[\u0027encrypted_text\u0027])\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Print text without sensitive data for a maximum of 5 rows where it\u0027s different from the original text\ncount \u003d 0\nfor index, row in df.iterrows():\n    if row[\u0027text\u0027] !\u003d row[\u0027text_without_sensitive_data\u0027] and row[\u0027text_without_sensitive_data\u0027] !\u003d \u0027\u0027 :\n        print(row[\u0027text_without_sensitive_data\u0027])\n        count +\u003d 1\n        if count \u003e\u003d 5:\n            break"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.2. Retrait des stopwords et des caractères spéciaux, et normalisation"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from nltk.corpus import stopwords\nimport nltk\n\n# Ensure the stopwords are downloaded\nnltk.download(\u0027stopwords\u0027)\n\nlang_dic \u003d {\n    \"en\": \"english\",\n    \"es\": \"spanish\",\n    \"de\": \"german\",\n    \"pl\": \"polish\",\n    \"fr\": \"french\",\n    \"pt\": \"portuguese\",\n    \"nl\": \"dutch\",\n    \"fa\": \"persian\",\n    \"ja\": \"japanese\",\n    \"ms\": \"malay\",\n    \"et\": \"estonian\",\n    \"vi\": \"vietnamese\",\n    \"ur\": \"urdu\",\n    \"cy\": \"welsh\",\n    \"cs\": \"czech\",\n    \"it\": \"italian\",\n    \"zh\": \"chinese\",\n    \"id\": \"indonesian\",\n    \"ru\": \"russian\",\n    \"sl\": \"slovene\",\n    \"ko\": \"korean\",\n    \"la\": \"latin\",\n    \"no\": \"norwegian\",\n    \"ro\": \"romanian\",\n    \"fi\": \"finnish\",\n    \"tl\": \"tagalog\",\n    \"uk\": \"ukrainian\",\n    \"hu\": \"hungarian\",\n    \"ca\": \"catalan\",\n    \"sv\": \"swedish\",\n    \"tr\": \"turkish\",\n    \"nds\": \"low_saxon\",\n    \"da\": \"danish\",\n    \"ta\": \"tamil\",\n    \"lb\": \"luxembourgish\",\n    \"bn\": \"bengali\",\n    \"mr\": \"marathi\",\n    \"eo\": \"esperanto\",\n    \"th\": \"thai\",\n    \"hi\": \"hindi\",\n    \"af\": \"afrikaans\",\n    \"sk\": \"slovak\",\n    \"so\": \"somali\",\n    \"is\": \"icelandic\",\n    \"br\": \"breton\",\n    \"te\": \"telugu\",\n    \"ar\": \"arabic\",\n    \"sh\": \"serbo_croatian\",\n    \"ceb\": \"cebuano\",\n    \"eu\": \"basque\",\n    \"kn\": \"kannada\",\n    \"ml\": \"malayalam\",\n    \"gl\": \"galician\",\n    \"qu\": \"quechua\",\n    \"gom\": \"goan_konkani\",\n    \"bs\": \"bosnian\",\n    \"war\": \"waray\",\n    \"sq\": \"albanian\",\n    \"el\": \"greek\",\n    \"sr\": \"serbian\",\n    \"az\": \"azerbaijani\",\n    \"lt\": \"lithuanian\",\n    \"dv\": \"divehi\",\n    \"si\": \"sinhala\",\n    \"kw\": \"cornish\",\n    \"fy\": \"frisian\",\n    \"he\": \"hebrew\",\n    \"ast\": \"asturian\",\n    \"kk\": \"kazakh\",\n    \"mk\": \"macedonian\",\n    \"rm\": \"romansh\",\n    \"hr\": \"croatian\",\n    \"lo\": \"lao\",\n    \"km\": \"khmer\",\n    \"su\": \"sundanese\",\n    \"ilo\": \"ilocano\",\n    \"lv\": \"latvian\",\n    \"ie\": \"interlingue\",\n    \"vo\": \"volapük\",\n    \"pms\": \"piedmontese\",\n    \"uz\": \"uzbek\",\n    \"ia\": \"interlingua\",\n    \"nn\": \"nynorsk\",\n    \"sw\": \"swahili\",\n    \"als\": \"alsatian\",\n    \"jv\": \"javanese\",\n    \"gu\": \"gujarati\",\n    \"tt\": \"tatar\",\n    \"oc\": \"occitan\",\n    \"ne\": \"nepali\",\n    \"jbo\": \"lojban\",\n    \"sco\": \"scots\",\n    \"ce\": \"chechen\",\n    \"ga\": \"irish\",\n    \"lmo\": \"lombard\",\n    \"ka\": \"georgian\",\n    \"vec\": \"venetian\",\n    \"mn\": \"mongolian\",\n    \"mg\": \"malagasy\",\n    \"hy\": \"armenian\",\n    \"bcl\": \"bikol\",\n    \"an\": \"aragonese\",\n    \"sd\": \"sindhi\",\n    \"wa\": \"walloon\",\n    \"io\": \"ido\",\n    \"li\": \"limburgish\",\n    \"my\": \"burmese\",\n    \"hsb\": \"upper_sorbian\",\n    \"bh\": \"bihari\",\n    \"as\": \"assamese\",\n    \"cbk\": \"chavacano\",\n    \"yo\": \"yoruba\",\n    \"mt\": \"maltese\",\n    \"gd\": \"scottish_gaelic\",\n    \"nah\": \"nahuatl\",\n    \"min\": \"minangkabau\",\n    \"tk\": \"turkmen\",\n    \"tg\": \"tajik\",\n    \"bar\": \"bavarian\",\n    \"ku\": \"kurdish\",\n    \"be\": \"belarusian\",\n    \"pa\": \"punjabi\",\n    \"new\": \"newari\"\n}\n\n# Define a function to remove stopwords from text based on language\ndef remove_stopwords(text, lang):\n    try:\n        # Use lang_dic to get the language name\n        language_name \u003d lang_dic.get(lang, \u0027english\u0027)\n        stop_words \u003d set(stopwords.words(language_name))\n    except OSError:\n        # If the language is not supported, default to English\n        stop_words \u003d set(stopwords.words(\u0027english\u0027))\n    words \u003d text.split()\n    filtered_words \u003d [word for word in words if word.lower() not in stop_words]\n    return \u0027 \u0027.join(filtered_words)\n\n# Apply the function to the \u0027text\u0027 column based on the \u0027language\u0027 column\ndf[\u0027text_without_sensitive_data\u0027] \u003d df.apply(lambda row: remove_stopwords(row[\u0027text_without_sensitive_data\u0027], row[\u0027language\u0027]), axis\u003d1)\ndf[\u0027encrypted_text\u0027] \u003d df.apply(lambda row: remove_stopwords(row[\u0027encrypted_text\u0027], row[\u0027language\u0027]), axis\u003d1)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Function to remove special characters from text\ndef remove_special_characters(text):\n    # Use regex to remove special characters\n    return re.sub(r\u0027[^a-zA-Z0-9\\s]\u0027, \u0027\u0027, text)\n\n# Apply the function to the \u0027text\u0027 column\ndf[\u0027text\u0027] \u003d df[\u0027text\u0027].apply(remove_special_characters)\ndf[\u0027text_without_sensitive_data\u0027] \u003d df[\u0027text_without_sensitive_data\u0027].apply(remove_special_characters)\ndf[\u0027encrypted_text\u0027] \u003d df[\u0027encrypted_text\u0027].apply(remove_special_characters)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Normalize the text\ndf[\u0027text\u0027] \u003d df[\u0027text\u0027].str.lower()\ndf[\u0027text_without_sensitive_data\u0027] \u003d df[\u0027text_without_sensitive_data\u0027].str.lower()\ndf[\u0027encrypted_text\u0027] \u003d df[\u0027encrypted_text\u0027].str.lower()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create one dataframe with encrypted sensitive data\ncleaned_tweets_encryption_df \u003d df.drop(columns\u003d[\u0027text\u0027, \u0027text_without_sensitive_data\u0027], errors\u003d\u0027ignore\u0027)\ncleaned_tweets_encryption_df \u003d cleaned_tweets_encryption_df.rename(columns\u003d{\u0027encrypted_text\u0027: \u0027text\u0027})\n\n# Create one dataframe with removed sensitive data\ncleaned_tweets_removal_df \u003d df.drop(columns\u003d[\u0027text\u0027, \u0027encrypted_text\u0027])\ncleaned_tweets_removal_df \u003d cleaned_tweets_removal_df.rename(columns\u003d{\u0027text_without_sensitive_data\u0027: \u0027text\u0027})"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Recipe outputs\ncleaned_tweets_encryption \u003d dataiku.Dataset(\"cleaned_tweets_encryption\")\ncleaned_tweets_encryption.write_with_schema(cleaned_tweets_encryption_df)\n\ncleaned_tweets_removal \u003d dataiku.Dataset(\"cleaned_tweets_removal\")\ncleaned_tweets_removal.write_with_schema(cleaned_tweets_removal_df)"
      ],
      "outputs": []
    }
  ]
}