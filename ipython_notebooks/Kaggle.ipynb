{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-venv-edf_sentiment_analysis",
      "display_name": "Python (env edf_sentiment_analysis)",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.9.20",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "creator": "francois.phe@dataiku.com",
    "tags": [],
    "customFields": {},
    "createdOn": 1741887169834,
    "modifiedBy": "francois.phe@dataiku.com"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nimport json\nimport os"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def configure_kaggle_api(kaggle_json):\n    \"\"\"Creates a Kaggle API configuration file programmatically.\"\"\"\n    kaggle_dir \u003d os.path.expanduser(\"~/.kaggle\")\n    os.makedirs(kaggle_dir, exist_ok\u003dTrue)\n    \n    kaggle_json_path \u003d os.path.join(kaggle_dir, \"kaggle.json\")\n    \n    # Save API key to kaggle.json\n    with open(kaggle_json_path, \"w\") as f:\n        json.dump(kaggle_json, f)\n    \n    # Set permissions\n    os.chmod(kaggle_json_path, 600)\n    \n    print(\"Kaggle API configured successfully!\")\n    print(kaggle_json_path)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "kaggle_json \u003d {\n    \"username\": \"franoisphe\",\n    \"key\": \"b7533c71dbbb8fb155b843fae45ec93a\"\n}"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "configure_kaggle_api(kaggle_json)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from kaggle.api.kaggle_api_extended import KaggleApi"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def download_dataset(dataset: str, destination: str \u003d \"./\"):\n    \"\"\"Downloads a dataset from Kaggle.\"\"\"\n    api \u003d KaggleApi()\n    api.authenticate()\n    \n    api.dataset_download_files(dataset, path\u003ddestination, unzip\u003dTrue)\n    print(f\"Dataset \u0027{dataset}\u0027 downloaded to {destination}\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example: Download a dataset\ndataset_name \u003d \"zynicide/wine-reviews\"\ndownload_dataset(dataset_name)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!ls /home/dssuser_francois1b07f560/"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "os.listdir(\"/home/dssuser_francois1b07f560/.config\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n\nos.environ[\u0027KAGGLE_USERNAME\u0027] \u003d \u0027franoisphe\u0027\nos.environ[\u0027KAGGLE_KEY\u0027] \u003d \u0027b7533c71dbbb8fb155b843fae45ec93a\u0027\n\nfrom kaggle import api # import the already authenticated API client\n\napi.dataset_download_files(\u0027netflix-inc/netflix-prize-data\u0027, path\u003d\u0027data\u0027, unzip\u003dTrue)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "os.listdir(\"~/\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nimport os\nimport tempfile\nfrom pathlib import Path\nimport shutil"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "folder \u003d dataiku.Folder(\"NvrBgKDk\")\n\n#Create temporary directory in /tmp\nwith tempfile.TemporaryDirectory() as tmpdirname:\n    #Loop through every file of the TF model and copy it localy to the tmp directory\n    for file_name in folder.list_paths_in_partition():\n        local_file_path \u003d tmpdirname + file_name\n        #Create file localy\n        if not os.path.exists(os.path.dirname(local_file_path)):\n            os.makedirs(os.path.dirname(local_file_path))\n        #Copy file from remote to local\n        with folder.get_download_stream(file_name) as f_remote, open(local_file_path,\u0027wb\u0027) as f_local:\n            shutil.copyfileobj(f_remote,f_local)\n    #Load model from local repository  \n    model \u003d tf.keras.models.load_model(os.path.join(tmpdirname,model_folder))   "
      ],
      "outputs": []
    }
  ]
}