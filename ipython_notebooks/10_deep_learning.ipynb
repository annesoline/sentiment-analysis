{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-venv-edf_sentiment_analysis",
      "display_name": "Python (env edf_sentiment_analysis)",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.9.20",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "modifiedBy": "anne-soline.guilbert-ly@dataiku.com",
    "customFields": {},
    "createdOn": 1743093624999,
    "creator": "anne-soline.guilbert-ly@dataiku.com",
    "tags": [
      "recipe-editor"
    ],
    "associatedRecipe": "recipe_from_notebook_10_deep_learning"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Packages"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nfrom dataiku import pandasutils as pdu\n\nimport pandas as pd\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay  \nfrom sklearn.metrics import classification_report  \n\nimport matplotlib.pyplot as plt  # Added import for plt\n\nfrom datetime import datetime\nimport os\nimport tempfile\nimport pickle\n\nfrom edf_commons.modelling import preprocess_data_for_dl\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Variables"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "LABEL_MAPPING \u003d {\u0027very negative\u0027: 0, \u0027negative\u0027: 1, \u0027neutral\u0027: 2, \u0027positive\u0027: 3, \u0027very positive\u0027: 4}\nINDEX_MAPPING \u003d {v: k for k, v in LABEL_MAPPING.items()}\nDATE_TIME \u003d datetime.now().strftime(\u0027%Y-%m-%d_%H-%M-%S\u0027)\n\n# Global variables\nproject \u003d dataiku.Project()\nvariables \u003d project.get_variables()\nARTEFACTS_FOLDER_ID \u003d variables[\"standard\"][\"artefacts_folder_id\"]\nCONFUSION_MATRICES_FOLDER_ID \u003d variables[\"standard\"][\"confusion_matrices_path\"]\nEPOCHS_PERF_FOLDER_ID \u003d variables[\"standard\"][\"epochs_perf_path\"]\nDL_MODELS_FOLDER_ID \u003d variables[\"standard\"][\"df_model_folder_id\"]\nDL_MODELS_DATA_FOLDER \u003d dataiku.Folder(DL_MODELS_FOLDER_ID)\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Input"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tweets_train \u003d dataiku.Dataset(\"tweets_train\")\ndf \u003d tweets_train.get_dataframe()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Learning"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Preprocess the data\ny \u003d df[\u0027label\u0027]\nX, y \u003d preprocess_data_for_dl(df[[\u0027tweet_length_chars\u0027, \u0027tweet_length_words\u0027, \u0027text\u0027]], y)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def save_image(img_name: str, folder_path: str)-\u003eNone:\n    \"\"\"Save the image of a graph recently plotted.\n    \n    ----------\n    Parameters\n        img_name: str\n            Name of the image given to the png file.\n        folder_path: str\n            Path to the sub folder within the managed folder.\n    \"\"\"\n\n    # Artefacts\n    fig_name \u003d f\"{img_name}_{DATE_TIME}.png\"\n    output_folder \u003d dataiku.Folder(ARTEFACTS_FOLDER_ID)\n    output_folder_path \u003d os.path.join(folder_path, fig_name)\n\n    with tempfile.TemporaryDirectory() as tmp_dir_name:\n        local_file_path \u003d os.path.join(tmp_dir_name, fig_name)\n        plt.savefig(fig_name)\n        output_folder.upload_file(output_folder_path, fig_name)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def apply_and_evaluate_deep_learning_model(X: pd.DataFrame, y: pd.Series) -\u003e tuple[pd.DataFrame, tf.keras.callbacks.History, pd.DataFrame]:\n    \"\"\"\n    Applies a deep learning model to the preprocessed data and evaluates its performance.\n\n    Parameters:\n    X (pd.DataFrame): The preprocessed data including numerical, categorical, and text data.\n    y (pd.Series): The labels.\n\n    Returns:\n    pd.DataFrame: A DataFrame containing the loss and accuracy of the model on the test data.\n    \"\"\"\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test \u003d train_test_split(X, y, test_size\u003d0.2, random_state\u003d42)\n    num_classes \u003d len(y.unique())\n\n    # Convert processed data to TensorFlow datasets\n    train_dataset \u003d tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values))\n    test_dataset \u003d tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values))\n\n    # Batch the datasets\n    train_dataset \u003d train_dataset.batch(32)\n    test_dataset \u003d test_dataset.batch(32)\n\n    # Initialize the model\n    model \u003d Sequential([\n        Embedding(input_dim\u003d10000, output_dim\u003d128),\n        LSTM(64, return_sequences\u003dTrue),\n        Dropout(0.5),\n        LSTM(64),\n        Dense(32, activation\u003d\u0027relu\u0027),\n        Dropout(0.5),\n        Dense(num_classes, activation\u003d\u0027softmax\u0027)  # Sigmoid for binary classification (positive/negative sentiment)\n    ])\n\n    # Compile the model\n    model.compile(optimizer\u003dAdam(learning_rate\u003d0.0005), \n                  loss\u003d\u0027sparse_categorical_crossentropy\u0027, \n                  metrics\u003d[\u0027accuracy\u0027])\n\n    # Train the model\n    history \u003d model.fit(train_dataset, epochs\u003d2, validation_data\u003dtest_dataset, verbose\u003d1)\n    \n    # Evaluate the model\n    loss, accuracy \u003d model.evaluate(test_dataset, verbose\u003d0)\n\n    # Obtenir les prédictions\n    y_pred \u003d model.predict(X_test.values)\n    y_pred_classes \u003d y_pred.argmax(axis\u003d1)\n\n    # Afficher la matrice\n    cm \u003d confusion_matrix(y_test, y_pred_classes)\n    disp \u003d ConfusionMatrixDisplay(confusion_matrix\u003dcm)\n    disp.plot()\n\n    # Save the image\n    save_image(\"confusion_matrix\", CONFUSION_MATRICES_FOLDER_ID)\n\n    # Générer le rapport\n    report_dict \u003d classification_report(y_test, y_pred_classes, output_dict\u003dTrue, target_names\u003dy_test.unique())\n    print(report_dict)\n\n    # Le convertir en DataFrame\n    report_df \u003d pd.DataFrame(report_dict).transpose()\n\n    # Save metrics into a DataFrame\n    metrics_df \u003d pd.DataFrame({\u0027average_loss\u0027: [loss], \u0027average_accuracy\u0027: [accuracy]})\n    \n    return metrics_df, history, report_df, model\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Evaluate model on encrypted data\nmetrics, history, report_df, dl_model \u003d apply_and_evaluate_deep_learning_model(X, y)\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save pickle"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Pipeline for encrypted data\ndl_artefact_name \u003d f\"df_{DATE_TIME}\"\nartefact_pickle_name \u003d f\"{dl_artefact_name}.pkl\"\n\nwith tempfile.TemporaryDirectory() as temp_dir:\n\n    local_file_path \u003d os.path.join(temp_dir, artefact_pickle_name)\n\n    with open(local_file_path, \u0027wb\u0027) as file:\n        pickle.dump(dl_model, file)\n\n    DL_MODELS_DATA_FOLDER.upload_file(artefact_pickle_name, local_file_path)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Outputs"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# remove last metrics\nreport_df \u003d report_df.iloc[:-3]\n# add model name column\nnn_artefact_name \u003d f\"lr_{DATE_TIME}\"\nreport_df[\"model\"] \u003d nn_artefact_name\n# add date time column\nreport_df[\"date_time\"] \u003d DATE_TIME\n# create label column by remapping the names\nreport_df[\u0027label\u0027] \u003d report_df.index.map(INDEX_MAPPING)\nfor col in metrics.columns:\n    report_df[col] \u003d metrics[col].iloc[0]\n    \n# drop the indexes\nreport_df.reset_index(drop\u003dTrue, inplace\u003dTrue)\n\n# Recipe outputs\ndl_metrics \u003d dataiku.Dataset(\"dl_metrics\")\ndl_metrics.write_with_schema(report_df)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot \u003d pd.DataFrame(history.history).plot()\nfig \u003d plot.get_figure()\nname \u003d f\"epochs_perf_evolution_{DATE_TIME}.png\"\nsave_image(\"epochs_perf_evolution_\", EPOCHS_PERF_FOLDER_ID)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    }
  ]
}