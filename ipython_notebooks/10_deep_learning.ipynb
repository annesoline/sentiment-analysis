{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-containerized-venv-edf_sentiment_analysis-cpu-xl-2-cpu-16gb-ram",
      "display_name": "Python in CPU-XL-2-cpu-16Gb-Ram (env edf_sentiment_analysis)",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.9.20",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "creator": "anne-soline.guilbert-ly@dataiku.com",
    "customFields": {},
    "createdOn": 1742813465542,
    "tags": [],
    "modifiedBy": "anne-soline.guilbert-ly@dataiku.com"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nfrom dataiku import pandasutils as pdu\nimport pandas as pd"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "prepared_tweets_encryption \u003d dataiku.Dataset(\"prepared_tweets_encryption\")\nprepared_tweets_encryption_df \u003d prepared_tweets_encryption.get_dataframe()\n\nprepared_tweets_removal \u003d dataiku.Dataset(\"prepared_tweets_removal\")\nprepared_tweets_removal_df \u003d prepared_tweets_removal.get_dataframe()\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# # 1) Settings\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# In this part: \n# - we import the packages that we need\n# - we define our parameters in particular the pre-trained model we want to use, the prediction type, the labels...\n# - we check the device available and set the default device accordingly (cpu or cuda)\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# ## a) Packages\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n# general packages\nimport dataiku\nfrom dataikuapi.dss.ml import DSSPredictionMLTaskSettings\n\nimport torch\nimport itertools\nimport os\nfrom datetime import datetime\nfrom sklearn.metrics import roc_auc_score, accuracy_score\n\n# transformers packages\nimport datasets\nimport transformers\nimport evaluate\n\n#import mlflow\nimport mlflow\nfrom mlflow.models.signature import ModelSignature\nfrom mlflow.types.schema import Schema, ColSpec\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# ## b) Parameters\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# This part should be modified to change parameters and adjust to your use case.\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n# HUGGING FACE PARAMETERS\nHF_MODEL \u003d \"bert-base-uncased\" # model name\nREVISION \u003d \"0a6aa9128b6194f4f3c4db429b6cb4891cdb421b\" # model revision (can be found in the commit)\nINF_BATCH_SIZE \u003d 16\n\n# PREDICTION PARAMETERS\nPREDICTION_TYPE \u003d \"MULTICLASS\" # Could be BINARY or REGRESSION\ntrain_data \u003d dataiku.Dataset(\"tweets_removal_train\").get_dataframe()\nCLASSES \u003d sorted(list(train_data[\u0027label\u0027].unique())) # labels should be sorted\nNUM_LABELS \u003d len(CLASSES)\nLABEL2ID \u003d {CLASSES[i]: i for i in range(NUM_LABELS)}\nID2LABEL \u003d {el:key for key,el in LABEL2ID.items()}\nTARGET \u003d \u0027labels_target\u0027\ntrain_data[\"labels_target\"] \u003d train_data[\"label_text\"].apply(lambda s: LABEL2ID[s])\nprint(\"Training dataset was loaded with classes: {} and target: {}\".format(CLASSES,TARGET))\n\n# ML-flow parameteters\nEXPERIMENT_FOLDER_ID \u003d \"zdw6Lwnn\"\nEXPERIMENT_NAME \u003d \"Experiment_finetuning\"\nMLFLOW_CODE_ENV_NAME \u003d \"edf_sentiment_analysis\"\nSAVED_MODEL_NAME \u003d \"model_finetuning\"\nARTIFACTS \u003d {SAVED_MODEL_NAME: \"hf_model.pth\"}\nTRAIN_DATASET \u003d \"train\"\nEVAL_DATASET \u003d \"test\"\nTARGET_NAME \u003d \u0027label_text\u0027 # in eval dataset\nDEPLOYMENT_METRIC \u003d \u0027eval_accuracy\u0027 # could be eval_roc_auc any other metric computed and logged in X tracking\nAUTO_DEPLOY\u003dTrue\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# ## c) Setting up ML_Flow\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# This part does not need to be modified.\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n# Create a mlflow_extension object\nclient \u003d dataiku.api_client()\nproject \u003d client.get_default_project()\nmlflow_extension \u003d project.get_mlflow_extension()\n\n# Get a handle on a Managed Folder to store the experiments.\nmanaged_folder \u003d project.get_managed_folder(EXPERIMENT_FOLDER_ID)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# ## d) (Opt) Garbage collect experiments - this will delete experiments that were removed\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# This part does not need to be modified.\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\nmlflow_extension.garbage_collect()\n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# ## e) Check Device\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# This part does not need to be modified.\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\nif torch.cuda.is_available():\n    print(\"Running on GPU\")\n    DEVICE \u003d \u0027cuda\u0027\nelse:\n    print(\u0027Running on CPU\u0027)\n    DEVICE \u003d \u0027cpu\u0027\n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# # 2) Load Data\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# In this part: \n# - we load the training and validation data and convert it to the [Hugging Face dataset](https://huggingface.co/docs/datasets/tabular_load#pandas-dataframes) format\n# - we tokenize the dataset\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# ## a) Load data\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# This part can be modified if you want to change the way training and validation sets are defined.\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n# Divide between train and test\ntrain_df \u003d train_data.sample(frac \u003d 0.8,random_state\u003d42)\ntest_df \u003d train_data.drop(train_df.index)\n\n# reset index\ntrain_df \u003d train_df.reset_index(drop\u003dTrue)\ntest_df \u003d test_df.reset_index(drop\u003dTrue)\n\n# Convert to Hugging Face Dataset format - convert to torch for GPU\nhf_train_dataset \u003d datasets.Dataset.from_pandas(train_df).class_encode_column(TARGET).with_format(\"torch\")\nhf_test_dataset \u003d datasets.Dataset.from_pandas(test_df).class_encode_column(TARGET).with_format(\"torch\")\nprint(\"Training and validation datasets loaded in Hugging Face Dataset format resp {} and {} rows\".format(len(hf_train_dataset),len(hf_test_dataset)))\n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# ## b) Tokenize\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# This part can be modified if you want to change the way the text is tokenized.\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n# define tokenizer + tokenizing function\ntokenizer \u003d transformers.AutoTokenizer.from_pretrained(HF_MODEL)\n\ndef preprocess_function(examples):\n    return tokenizer(examples[\"text\"], truncation\u003dTrue,padding\u003d\"max_length\")\n\n# apply tokenizing function on train and test set\ntokenized_train \u003d hf_train_dataset.map(preprocess_function, batched\u003dTrue)\ntokenized_test \u003d hf_test_dataset.map(preprocess_function, batched\u003dTrue)\nprint(\"Train and test sets were tokenized\")\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# # 3) Train Model + log in Experiment Tracking\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# In this part: \n# - we define a hyperparameters grid \n# - we add a metric of interest that we want to log at training time (could be anything but here we chose ROC-AUC)\n# - we start the training with a layer of experiment tracking and in the end we log the model as a ML-Flow model. \n# \n# On the last point as there is no ML-flow flavor for Hugging Face we use [this approach](https://julsimon.medium.com/using-mlflow-with-hugging-face-transformers-4f69093a6c04) with callbacks to make sure the logs are properly sent to the Experiment Tracking Section. To log the model, we define a pyfunc variant that relies on a [Hugging Face pipeline](https://huggingface.co/docs/transformers/pipeline_tutorial) and that was inspired by [this tutorial](https://developer.dataiku.com/latest/tutorials/machine-learning/experiment-tracking/keras-nlp/index.html). This class will embed the preprocessing, prediction and post-processing. \n# \n# During this section, all the experiments are logged in the Experiment Tracking section associated with the Experiments folder.\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# ## a) Define hyperparameter grid- change grid\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# This part should be modified to define the hyperparameters you wish you test in your experiment.\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\nparam_grid \u003d {\n    \u0027batch_size\u0027:[8,16],\n    \u0027learning_rate\u0027:[2e-5,4e-5],\n    \u0027num_train_epochs\u0027:[5,20,50],\n    \u0027training_dir\u0027:\"_\"\n}\n\nall_params \u003d [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n\nprint(\"{} combinations will be tried. \\n This corresponds to this grid: {}\".format(len(all_params),param_grid))\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# ## b) Define training metric\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# This part can be modified if you want to change the metrics that will be logged. You could use any training metric you like to chose the model you want to deploy, here we go for ROC AUC and accuracy.\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\ndef compute_metrics(model,tokenized_test_dataset):\n    # retrieve model prediction\n    pred\u003dmodel.predict(tokenized_test_dataset)\n    # true labels\n    true_labels\u003dpred.label_ids\n    # predictions\n    predictions\u003dpred.predictions\n    predictions\u003dtorch.from_numpy(predictions)\n\n    # retrieve scores and predicted labels from predictions\n    pred_scores\u003dtorch.nn.functional.softmax(input\u003dpredictions, dim\u003d-1)\n    pred_labels\u003dtorch.argmax(predictions, dim\u003d1)\n\n    # compute scores\n    auc\u003droc_auc_score(y_true\u003dtrue_labels, y_score\u003dpred_scores,multi_class\u003d\u0027ovr\u0027, average\u003d\u0027macro\u0027)\n    accuracy\u003daccuracy_score(y_true\u003dtrue_labels,y_pred\u003dpred_labels)\n    return {\"auc\": auc,\"accuracy\": accuracy}\n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# ## c) Training\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# This part can be modified if you want to change the training arguments or the metrics that are logged.\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\nwith project.setup_mlflow(managed_folder) as mlflow:\n\n    # (1) SET-UP THE EXPERIMENT\n    \u0027\u0027\u0027If the experiment did not already exist, we create it\u0027\u0027\u0027\n    if len(mlflow_extension.list_experiments()) \u003e\u003d 1:\n        if EXPERIMENT_NAME in [exp[\u0027name\u0027] for exp in mlflow_extension.list_experiments()[\u0027experiments\u0027]]:\n            print(\"Experiment already existed with name : {}\".format(EXPERIMENT_NAME))\n        else:\n            experiment_id \u003d mlflow.create_experiment(EXPERIMENT_NAME)\n            print(\"Experiment created with name : {}\".format(EXPERIMENT_NAME))\n    else:\n        experiment_id \u003d mlflow.create_experiment(EXPERIMENT_NAME)\n        print(\"Experiment created with name : {}\".format(EXPERIMENT_NAME))\n\n    experiment \u003d mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n    experiment_id \u003d experiment.experiment_id\n    print(\"Experiment name is: {} and ID is: {} \".format(EXPERIMENT_NAME,experiment_id))\n\n    mlflow.tracking.MlflowClient().set_experiment_tag(experiment_id, \"library\", \"hugging_face\")\n    print(\u0027Tags added to the experiment\u0027)\n\n\n    # (2) START LOOPING\n    for i,params in enumerate(all_params):\n        # (a) Start a run\n        with mlflow.start_run(experiment_id\u003dexperiment_id) as run:\n            run_id \u003d run.info.run_id\n            print(f\u0027Starting run {run_id} ...\\n{params}\u0027)\n            print(\"Iteration {} out of {}\".format(i+1,len(all_params)))\n\n            #HF callbacks\n            # retrieve newly created experiment\n            os.environ[\"MLFLOW_EXPERIMENT_NAME\"] \u003d EXPERIMENT_NAME\n            # flatten the parameters dictionary before logging\n            os.environ[\"MLFLOW_FLATTEN_PARAMS\"] \u003d \"1\"\n            # we do not need to log the checkpoint as we will save the best model right after\n            os.environ[\"HF_MLFLOW_LOG_ARTIFACTS\"] \u003d \"0\"\n\n            # instantiate the model\n            model \u003d transformers.AutoModelForSequenceClassification.from_pretrained(HF_MODEL,num_labels\u003dNUM_LABELS,id2label\u003dID2LABEL,label2id\u003dLABEL2ID,revision\u003dREVISION).to(DEVICE)\n            print(\u0027Model instantiated on device {}\u0027.format(model.device))\n\n            # define training arguments - can be changed\n            training_args \u003d transformers.TrainingArguments(output_dir\u003dparams[\u0027training_dir\u0027],\n                                          learning_rate\u003dparams[\u0027learning_rate\u0027],\n                                          logging_strategy\u003d\u0027epoch\u0027,\n                                          per_device_train_batch_size\u003dparams[\u0027batch_size\u0027],\n                                          per_device_eval_batch_size\u003dparams[\u0027batch_size\u0027],\n                                          num_train_epochs\u003dparams[\u0027num_train_epochs\u0027],\n                                          weight_decay\u003d0.01,\n                                          evaluation_strategy\u003d\"epoch\",\n                                          save_strategy\u003d\"epoch\",\n                                          load_best_model_at_end\u003dTrue,\n                                          push_to_hub\u003dFalse,metric_for_best_model\u003d\u0027eval_loss\u0027)\n\n            # implement early stopping\n            early_stopper \u003d transformers.EarlyStoppingCallback(early_stopping_patience\u003d3,\n                                                             early_stopping_threshold\u003d0.05)\n\n            # define trainer\n            trainer \u003d transformers.Trainer(model\u003dmodel,\n                                           args\u003dtraining_args,\n                                           train_dataset\u003dtokenized_train,\n                                           eval_dataset\u003dtokenized_test,\n                                           tokenizer\u003dtokenizer,\n                                           callbacks\u003d[early_stopper]\n                                           )\n            # this will start a run\n            trainer.train()\n\n            # we log the roc auc on the eval set - can be changed\n            metrics \u003d compute_metrics(trainer,tokenized_test)\n            mlflow.log_metric(\"eval_roc_auc\",metrics[\u0027auc\u0027])\n            mlflow.log_metric(\"eval_accuracy\",metrics[\u0027accuracy\u0027])\n\n            # save the best model (best model was loaded at end)\n            trainer.save_model(ARTIFACTS.get(SAVED_MODEL_NAME))\n\n            # this class based on the PythonModel flavor allows us to easily package our pipeline\n            class HF_Wrapper(mlflow.pyfunc.PythonModel):\n\n                def load_context(self, context):\n                    import transformers\n\n                    # retrieve model path\n                    model_path\u003dcontext.artifacts[SAVED_MODEL_NAME]\n                    # load model and tokenizer\n                    self.tokenizer\u003dtransformers.DistilBertTokenizer.from_pretrained(model_path) # tokenizer\n                    self.model \u003d transformers.AutoModelForSequenceClassification.from_pretrained(model_path) # model\n\n\n                def predict(self, context, model_input):\n                    import tqdm\n                    import numpy as np\n                    import transformers\n                    import datasets\n                    from transformers.pipelines.base import KeyDataset\n\n                    # convert the data to hf format\n                    hf_data \u003d datasets.Dataset.from_pandas(model_input)\n\n                    # pipe with scores - tokenizer arguments can be changed\n                    pipe \u003d transformers.TextClassificationPipeline(model\u003dself.model, tokenizer\u003dself.tokenizer, return_all_scores\u003dTrue)\n                    tokenizer_kwargs \u003d {\u0027padding\u0027:True,\u0027truncation\u0027:True,\u0027max_length\u0027:512}\n\n                    # create output proba\n                    probas \u003d np.empty([len(hf_data),NUM_LABELS])\n\n                    for i,proba_array in tqdm.tqdm(enumerate(pipe(KeyDataset(hf_data,\u0027text\u0027),batch_size\u003dINF_BATCH_SIZE,**tokenizer_kwargs))):\n                        probas[i]\u003d[class_dict[\u0027score\u0027] for class_dict in proba_array]\n\n                    return np.array(probas)\n\n            # define signature of the model\n            input_schema \u003d Schema([ColSpec(\"string\", \"text\")]) # our input is a text column\n            output_schema \u003d Schema([ColSpec(\"float\")])  # our output is a probability\n            signature \u003d ModelSignature(inputs\u003dinput_schema, outputs\u003doutput_schema)\n\n            # define the artifact path where ml_flow model will be stored\n            artifact_path \u003d str(run_id)+\u0027/\u0027\n            mlflow.pyfunc.log_model(artifact_path\u003dartifact_path,python_model\u003dHF_Wrapper(),artifacts\u003dARTIFACTS,\n                                    signature\u003dsignature)\n            print(\"Model logged\")\n            mlflow_extension.set_run_inference_info(run_id\u003drun_id,\n                                                    prediction_type\u003dPREDICTION_TYPE,\n                                                    classes\u003dCLASSES,\n                                                    code_env_name\u003dMLFLOW_CODE_ENV_NAME,target\u003dTARGET_NAME)\n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# # 4) Deploy Model with best Deployment metric\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# This part does not need to be modified.\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# In this part:\n# - we find the run in the experiment that resulted in the best AUC;\n# - we deploy the corresponding model as a  saved model in the Flow. When clicking on the last active version, you will have access to performance assets such as confusion matrix, lift charts etc.\n# \n# This choice could be overriden by the user as it is possible to deploy any of the runs directly from the Experiment Tracking Section.\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\nif AUTO_DEPLOY:\n    mlflow_handle \u003d project.setup_mlflow(managed_folder\u003dEXPERIMENT_FOLDER_ID)\n\n    # Get experiment\n    experiment \u003d mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n    print(\"Experiment: \", experiment)\n\n    # List the runs and get the one with the best accuracy score\n    print(\"Looking for the best run within the experiment\")\n    best_run \u003d None\n    for run_info in mlflow.list_run_infos(experiment.experiment_id):\n        run \u003d mlflow.get_run(run_info.run_id)\n        if best_run is None:\n            best_run \u003d mlflow.get_run(run_info.run_id)\n        elif run.data.metrics.get(DEPLOYMENT_METRIC, 0) \u003e best_run.data.metrics.get(DEPLOYMENT_METRIC, 0):\n            best_run \u003d run\n    print(f\"Run id {best_run.info.run_id} with {DEPLOYMENT_METRIC}\u003d{best_run.data.metrics.get(DEPLOYMENT_METRIC)}\")\n\n    # Deploy the model on the flow\n    run_id \u003d best_run.info.run_id\n    prediction_type \u003d DSSPredictionMLTaskSettings.PredictionTypes.BINARY\n\n    # Get or create the Saved Model\n    sm_id \u003d None\n    for sm in project.list_saved_models():\n        if sm[\"name\"] !\u003d SAVED_MODEL_NAME:\n            continue\n        else:\n            sm_id \u003d sm[\"id\"]\n            print(f\"Found Saved Model {sm[\u0027name\u0027]} with id {sm[\u0027id\u0027]}\")\n            break\n    if sm_id:\n        saved_model \u003d project.get_saved_model(sm_id)\n    else:\n        saved_model \u003d project.create_mlflow_pyfunc_model(SAVED_MODEL_NAME, PREDICTION_TYPE)\n        sm_id \u003d saved_model.id\n        print(f\"Saved Model not found, created new one with id {sm_id}\")\n\n    # Define model version\n    model_versions\u003d[model[\u0027id\u0027] for model in saved_model.list_versions()]\n\n    if model_versions is None:\n        version_id \u003d \"finetuning_v1\"\n    else:\n        i\u003d1\n        while \"finetuning_v{}\".format(i) in model_versions:\n            i+\u003d1\n        version_id \u003d \"finetuning_v{}\".format(i)\n\n    print(f\"Deploying the model {SAVED_MODEL_NAME} on the flow and running evaluation with dataset {EVAL_DATASET}\")\n    sm_external_model_version_handler \u003d mlflow_extension.deploy_run_model(\n        run_id,\n        sm_id,\n        evaluation_dataset\u003dEVAL_DATASET,\n        version_id\u003dversion_id,\n        target_column_name\u003dTARGET_NAME\n    )\nelse:\n    print(\u0027No model deployed - experiments are available in the X-Tracking Section\u0027)"
      ],
      "outputs": []
    }
  ]
}