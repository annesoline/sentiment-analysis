{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-venv-edf_sentiment_analysis",
      "display_name": "Python (env edf_sentiment_analysis)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.20",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "hide_input": false,
    "creator": "anne-soline.guilbert-ly@dataiku.com",
    "dkuGit": {
      "lastInteraction": 1742230932865,
      "gitReference": {
        "remote": "git@github.com:annesoline/sentiment-analysis.git",
        "checkout": "refs/heads/main",
        "remotePath": "check_sensible_data.ipynb",
        "remoteLogin": "",
        "lastHash": "804e5cfc33c9ef3e4f1d1489c9df687ccb0c5950",
        "lastTimestamp": 1742230879000,
        "isDirty": false
      }
    },
    "customFields": {},
    "createdOn": 1742230932864,
    "tags": [],
    "modifiedBy": "anne-soline.guilbert-ly@dataiku.com"
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pd.set_option(\u0027display.max_colwidth\u0027, None)\n# Load the required libraries\nimport os\nimport torch\nimport dataiku\nimport pandas as pd\nfrom transformers import pipeline\nfrom dataiku import pandasutils as pdu\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "enhanced_tweets_informations \u003d dataiku.Dataset(\"enhanced_tweets_informations\")\ndf \u003d enhanced_tweets_informations.get_dataframe()\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "hf_transformers_home_dir \u003d os.getenv(\"HF_HOME\")\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(hf_transformers_home_dir)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# Read recipe inputs\nenhanced_tweets_informations \u003d dataiku.Dataset(\"enhanced_tweets_informations\")\ndf \u003d enhanced_tweets_informations.get_dataframe()\n\n\n# Compute recipe outputs from inputs\n# Define the model to use\nmodel_name \u003d \"dslim/bert-base-NER\"\n\ndef perform_ner_inference(model_name, input_df):\n    \"\"\"\n    perform_ner_inference performs NER inference on a dataframe using a specified Hugging Face model.\n    \n    :param model_name: The name of the Hugging Face model to use for NER.\n    :param input_df: The input dataframe with at least two columns, document_id and text.\n    :return: pd.DataFrame. A dataframe containing the NER results, with at least columns \"document_id\", \"text\", and \"predicted_labels\".\n    \"\"\"\n    # Load the pre-trained tokenizer and model\n    tokenizer \u003d AutoTokenizer.from_pretrained(model_name, cache_dir\u003dhf_transformers_home_dir)\n    model \u003d AutoModelForTokenClassification.from_pretrained(model_name, cache_dir\u003dhf_transformers_home_dir)\n\n    # Load the token classification pipeline\n    token_classification_pipeline \u003d pipeline(\"ner\", model\u003dmodel, tokenizer\u003dtokenizer, aggregation_strategy\u003d\"first\") # pass device\u003d0 if using gpu\n\n    # Perform token classification on each row of the dataframe\n    predicted_labels \u003d []\n    for index, row in df.iterrows():\n        document_id \u003d row[\"id\"]\n        text \u003d row[\"text\"]\n        results \u003d token_classification_pipeline(text)\n        predicted_labels.append(results)\n        \n    df[\u0027predicted_labels\u0027] \u003d predicted_labels\n\n    return df\n\ndocument_scored_df \u003d perform_ner_inference(model_name, df)\n\ndocument_scored_df.head()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\ntweets_NER_with_Python_code_df \u003d document_scored_df # For this sample code, simply copy input to output\n\n\n# Write recipe outputs\ntweets_NER_with_Python_code \u003d dataiku.Dataset(\"tweets_NER_with_Python_code\")\ntweets_NER_with_Python_code.write_with_schema(tweets_NER_with_Python_code_df)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Too long to load"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import nltk\nfrom nltk import ne_chunk, pos_tag, word_tokenize\nfrom nltk.chunk import conlltags2tree, tree2conlltags\nnltk.download(\u0027maxent_ne_chunker_tab\u0027)\nnltk.download(\u0027words\u0027)\n\n# Function to extract named entities from text\ndef extract_entities(text):\n    words \u003d word_tokenize(text)\n    tagged \u003d pos_tag(words)\n    entities \u003d ne_chunk(tagged)\n    return entities\n\n# Apply the function to the text column in your DataFrame\ndf \u003d df.sample(n\u003d10, random_state\u003d42)\n\ndf[\u0027named_entities\u0027] \u003d df[\u0027text\u0027].apply(extract_entities)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df.head()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    }
  ]
}