{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-containerized-venv-edf_sentiment_analysis-cpu-xl-2-cpu-16gb-ram",
      "display_name": "Python in CPU-XL-2-cpu-16Gb-Ram (env edf_sentiment_analysis)",
      "language": "python"
    },
    "hide_input": false,
    "creator": "anne-soline.guilbert-ly@dataiku.com",
    "customFields": {},
    "createdOn": 1742824723900,
    "tags": [],
    "modifiedBy": "anne-soline.guilbert-ly@dataiku.com"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport tensorflow as tf\n\ndef preprocess_data(X: pd.DataFrame) -\u003e pd.DataFrame:\n    \"\"\"\n    Preprocesses the data by handling categorical variables with one-hot encoding\n    and numerical columns with standard scaling.\n\n    Parameters:\n    X (pd.DataFrame): The data to preprocess.\n\n    Returns:\n    pd.DataFrame: The processed data.\n    \"\"\"\n    # Handle categorical variables with one-hot encoding\n    categorical_features \u003d [\u0027date\u0027, \u0027user\u0027, \u0027language\u0027]\n    encoder \u003d OneHotEncoder(sparse_output\u003dFalse, handle_unknown\u003d\u0027ignore\u0027)\n    \n    X_categorical \u003d encoder.fit_transform(X[categorical_features])\n    \n    # Handle numerical columns with standard scaling\n    numerical_features \u003d X.drop(columns\u003dcategorical_features).columns\n    scaler \u003d StandardScaler()\n    \n    X_numerical \u003d scaler.fit_transform(X[numerical_features])\n    \n    # Concatenate processed categorical and numerical features\n    X_processed \u003d pd.concat([pd.DataFrame(X_numerical), pd.DataFrame(X_categorical)], axis\u003d1)\n    \n    return X_processed\n\ndef apply_and_evaluate_deep_learning_model(X: pd.DataFrame, y: pd.Series) -\u003e pd.DataFrame:\n    \"\"\"\n    Applies a deep learning model to the preprocessed data and evaluates its performance.\n\n    Parameters:\n    X (pd.DataFrame): The preprocessed data.\n    y (pd.Series): The labels.\n\n    Returns:\n    pd.DataFrame: A DataFrame containing the loss and accuracy of the model on the test data.\n    \"\"\"\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test \u003d train_test_split(X, y, test_size\u003d0.2, random_state\u003d42)\n\n    # Preprocess the data\n    X_train_processed \u003d preprocess_data(X_train)\n    X_test_processed \u003d preprocess_data(X_test)\n    input_shape \u003d X_train_processed.shape[1]\n\n    # Convert processed data to TensorFlow datasets\n    train_dataset \u003d tf.data.Dataset.from_tensor_slices((X_train_processed.values, y_train.values))\n    test_dataset \u003d tf.data.Dataset.from_tensor_slices((X_test_processed.values, y_test.values))\n\n    # Batch the datasets\n    train_dataset \u003d train_dataset.batch(32)\n    test_dataset \u003d test_dataset.batch(32)\n\n    # Initialize the model\n    model \u003d Sequential([\n        Dense(128, activation\u003d\u0027relu\u0027, input_shape\u003d(input_shape,)),\n        Dropout(0.5),\n        Dense(64, activation\u003d\u0027relu\u0027),\n        Dropout(0.5),\n        Dense(1, activation\u003d\u0027sigmoid\u0027)\n    ])\n\n    # Compile the model\n    model.compile(optimizer\u003dAdam(learning_rate\u003d0.001), \n                  loss\u003d\u0027binary_crossentropy\u0027, \n                  metrics\u003d[\u0027accuracy\u0027])\n\n    # Train the model\n    model.fit(train_dataset, epochs\u003d10, validation_data\u003dtest_dataset, verbose\u003d1)\n\n    # Evaluate the model\n    loss, accuracy \u003d model.evaluate(test_dataset, verbose\u003d0)\n    \n    # Save metrics into a DataFrame\n    metrics_df \u003d pd.DataFrame({\u0027loss\u0027: [loss], \u0027accuracy\u0027: [accuracy]})\n    \n    return metrics_df\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nfrom dataiku import pandasutils as pdu\nimport pandas as pd"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example: load a DSS dataset as a Pandas dataframe\nmydataset \u003d dataiku.Dataset(\"mydataset\")\nmydataset_df \u003d mydataset.get_dataframe()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    }
  ]
}