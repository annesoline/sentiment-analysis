{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-venv-edf_sentiment_analysis",
      "display_name": "Python (env edf_sentiment_analysis)",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.9.20",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "modifiedBy": "francois.phe@dataiku.com",
    "customFields": {},
    "createdOn": 1742824723900,
    "creator": "anne-soline.guilbert-ly@dataiku.com",
    "tags": []
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nfrom dataiku import pandasutils as pdu\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nimport pandas as pd\nimport tensorflow as tf"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "prepared_tweets_encryption \u003d dataiku.Dataset(\"prepared_tweets_encryption\")\ndf \u003d prepared_tweets_encryption.get_dataframe()\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df[\"text\"]"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import class_weight\nimport numpy as np\n\n\ndef preprocess_data(X: pd.DataFrame, y: pd.Series) -\u003e tuple[pd.DataFrame, pd.Series]:\n    \"\"\"\n    Preprocesses the data by handling categorical variables with one-hot encoding,\n    numerical columns with standard scaling, and text data with tokenization and padding.\n\n    Parameters:\n    X (pd.DataFrame): The data to preprocess.\n    y (pd.Series): The label to encode.\n\n    Returns:\n    pd.DataFrame: The processed data including numerical, categorical, and text data.\n    pd.Series: The processed label.\n    \"\"\"\n\n    # Handle numerical columns with standard scaling\n    numerical_features \u003d [\u0027tweet_length_chars\u0027, \u0027tweet_length_words\u0027]\n    scaler \u003d StandardScaler()\n    X_numerical \u003d scaler.fit_transform(X[numerical_features])\n\n    # Handle text data with tokenization and padding\n    tokenizer \u003d Tokenizer(num_words\u003d5000)\n    tokenizer.fit_on_texts(X[\u0027text\u0027].astype(str))  # Ensure text data is string\n    X_text \u003d tokenizer.texts_to_sequences(X[\u0027text\u0027].astype(str))  # Ensure text data is string\n    X_text \u003d pad_sequences(X_text, maxlen\u003d100)\n\n    # Concatenate processed categorical, numerical, and text features\n    X_processed \u003d pd.concat([pd.DataFrame(X_numerical, columns\u003dnumerical_features), pd.DataFrame(X_text)], axis\u003d1)\n\n    label_encoder \u003d LabelEncoder()\n    y \u003d pd.Series(label_encoder.fit_transform(y))\n\n    return X_processed, y\n\n\ndef apply_and_evaluate_deep_learning_model(X: pd.DataFrame, y: pd.Series) -\u003e tuple[pd.DataFrame, tf.keras.callbacks.History, pd.DataFrame]:\n    \"\"\"\n    Applies a deep learning model to the preprocessed data and evaluates its performance.\n\n    Parameters:\n    X (pd.DataFrame): The preprocessed data including numerical, categorical, and text data.\n    y (pd.Series): The labels.\n\n    Returns:\n    pd.DataFrame: A DataFrame containing the loss and accuracy of the model on the test data.\n    \"\"\"\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test \u003d train_test_split(X, y, test_size\u003d0.2, random_state\u003d42)\n    num_classes \u003d len(y.unique())\n\n    # Convert processed data to TensorFlow datasets\n    train_dataset \u003d tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values))\n    test_dataset \u003d tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values))\n\n    # Batch the datasets\n    train_dataset \u003d train_dataset.batch(32)\n    test_dataset \u003d test_dataset.batch(32)\n\n    # Initialize the model\n    model \u003d Sequential([\n        Embedding(input_dim\u003d10000, output_dim\u003d128, input_length\u003d100),\n        LSTM(64, return_sequences\u003dTrue),\n        Dropout(0.5),\n        LSTM(64),\n        Dense(32, activation\u003d\u0027relu\u0027),\n        Dropout(0.5),\n        Dense(num_classes, activation\u003d\u0027softmax\u0027)  # Sigmoid for binary classification (positive/negative sentiment)\n    ])\n\n    # Compile the model\n    model.compile(optimizer\u003dAdam(learning_rate\u003d0.0005), \n                  loss\u003d\u0027sparse_categorical_crossentropy\u0027, \n                  metrics\u003d[\u0027accuracy\u0027])\n\n    # weights \u003d class_weight.compute_class_weight(\n    #     class_weight\u003d\u0027balanced\u0027,\n    #     classes\u003dnp.unique(y_train),\n    #     y\u003dy_train\n    # )\n\n    # class_weights \u003d dict(enumerate(weights))\n\n    # Train the model\n    history \u003d model.fit(train_dataset, epochs\u003d200, validation_data\u003dtest_dataset, verbose\u003d1)\n    \n    # Evaluate the model\n    loss, accuracy \u003d model.evaluate(test_dataset, verbose\u003d0)\n\n    # Obtenir les prédictions\n    y_pred \u003d model.predict(X_test.values)\n    y_pred_classes \u003d y_pred.argmax(axis\u003d1)\n\n    # Afficher la matrice\n    cm \u003d confusion_matrix(y_test, y_pred_classes)\n    disp \u003d ConfusionMatrixDisplay(confusion_matrix\u003dcm)\n    disp.plot()\n\n    # Save the image\n    plt.savefig(\u0027confusion_matrix.png\u0027)\n\n    # Générer le rapport\n    report_dict \u003d classification_report(y_test, y_pred_classes, output_dict\u003dTrue)\n\n    # Le convertir en DataFrame\n    report_df \u003d pd.DataFrame(report_dict).transpose()\n\n    # Save metrics into a DataFrame\n    metrics_df \u003d pd.DataFrame({\u0027loss\u0027: [loss], \u0027accuracy\u0027: [accuracy]})\n    \n    return metrics_df, history, report_df\n\n# Evaluate model on encrypted data\ny_encrypted \u003d df[\u0027label\u0027]\n\nX_encrypted, y_encrypted \u003d preprocess_data(df[[\u0027tweet_length_chars\u0027, \u0027tweet_length_words\u0027, \u0027text\u0027]], y_encrypted)\n\nmetrics_encrypted, history, report_df \u003d apply_and_evaluate_deep_learning_model(df[\"text\"], y_encrypted)\nprint(f\"Metrics on encrypted data:\")\nmetrics_encrypted.head()\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "scrolled": false
      },
      "source": [
        "from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import class_weight\nimport numpy as np\n\n\ndef preprocess_data(X: pd.DataFrame, y: pd.Series) -\u003e tuple[pd.DataFrame, pd.Series]:\n    \"\"\"\n    Preprocesses the data by handling categorical variables with one-hot encoding,\n    numerical columns with standard scaling, and text data with tokenization and padding.\n\n    Parameters:\n    X (pd.DataFrame): The data to preprocess.\n    y (pd.Series): The label to encode.\n\n    Returns:\n    pd.DataFrame: The processed data including numerical, categorical, and text data.\n    pd.Series: The processed label.\n    \"\"\"\n\n    # Handle numerical columns with standard scaling\n    numerical_features \u003d [\u0027tweet_length_chars\u0027, \u0027tweet_length_words\u0027]\n    scaler \u003d StandardScaler()\n    X_numerical \u003d scaler.fit_transform(X[numerical_features])\n\n    # Handle text data with tokenization and padding\n    tokenizer \u003d Tokenizer(num_words\u003d5000)\n    tokenizer.fit_on_texts(X[\u0027text\u0027].astype(str))  # Ensure text data is string\n    X_text \u003d tokenizer.texts_to_sequences(X[\u0027text\u0027].astype(str))  # Ensure text data is string\n    X_text \u003d pad_sequences(X_text, maxlen\u003d100)\n\n    # Concatenate processed categorical, numerical, and text features\n    X_processed \u003d pd.concat([pd.DataFrame(X_numerical, columns\u003dnumerical_features), pd.DataFrame(X_text)], axis\u003d1)\n\n    label_encoder \u003d LabelEncoder()\n    y \u003d pd.Series(label_encoder.fit_transform(y))\n\n    return X_processed, y\n\n\ndef apply_and_evaluate_deep_learning_model(X: pd.DataFrame, y: pd.Series) -\u003e tuple[pd.DataFrame, tf.keras.callbacks.History, pd.DataFrame]:\n    \"\"\"\n    Applies a deep learning model to the preprocessed data and evaluates its performance.\n\n    Parameters:\n    X (pd.DataFrame): The preprocessed data including numerical, categorical, and text data.\n    y (pd.Series): The labels.\n\n    Returns:\n    pd.DataFrame: A DataFrame containing the loss and accuracy of the model on the test data.\n    \"\"\"\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test \u003d train_test_split(X, y, test_size\u003d0.2, random_state\u003d42)\n    num_classes \u003d len(y.unique())\n\n    # Convert processed data to TensorFlow datasets\n    train_dataset \u003d tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values))\n    test_dataset \u003d tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values))\n\n    # Batch the datasets\n    train_dataset \u003d train_dataset.batch(32)\n    test_dataset \u003d test_dataset.batch(32)\n\n    # Initialize the model\n    model \u003d Sequential([\n        Dense(64, activation\u003d\u0027relu\u0027, input_shape\u003d(X_train.shape[1],)),\n        Dropout(0.2),\n        Dense(128, activation\u003d\u0027relu\u0027),\n        Dropout(0.2),\n        Dense(num_classes, activation\u003d\u0027softmax\u0027)\n    ])\n\n    # Compile the model\n    model.compile(optimizer\u003dAdam(learning_rate\u003d0.0005), \n                  loss\u003d\u0027sparse_categorical_crossentropy\u0027, \n                  metrics\u003d[\u0027accuracy\u0027])\n\n    # weights \u003d class_weight.compute_class_weight(\n    #     class_weight\u003d\u0027balanced\u0027,\n    #     classes\u003dnp.unique(y_train),\n    #     y\u003dy_train\n    # )\n\n    # class_weights \u003d dict(enumerate(weights))\n\n    # Train the model\n    history \u003d model.fit(train_dataset, epochs\u003d200, validation_data\u003dtest_dataset, verbose\u003d1)\n    \n    # Evaluate the model\n    loss, accuracy \u003d model.evaluate(test_dataset, verbose\u003d0)\n\n    # Obtenir les prédictions\n    y_pred \u003d model.predict(X_test.values)\n    y_pred_classes \u003d y_pred.argmax(axis\u003d1)\n\n    # Afficher la matrice\n    cm \u003d confusion_matrix(y_test, y_pred_classes)\n    disp \u003d ConfusionMatrixDisplay(confusion_matrix\u003dcm)\n    disp.plot()\n\n    # Save the image\n    plt.savefig(\u0027confusion_matrix.png\u0027)\n\n    # Générer le rapport\n    report_dict \u003d classification_report(y_test, y_pred_classes, output_dict\u003dTrue)\n\n    # Le convertir en DataFrame\n    report_df \u003d pd.DataFrame(report_dict).transpose()\n\n    # Save metrics into a DataFrame\n    metrics_df \u003d pd.DataFrame({\u0027loss\u0027: [loss], \u0027accuracy\u0027: [accuracy]})\n    \n    return metrics_df, history, report_df\n\n# Evaluate model on encrypted data\ny_encrypted \u003d df[\u0027label\u0027]\n\nX_encrypted, y_encrypted \u003d preprocess_data(df[[\u0027tweet_length_chars\u0027, \u0027tweet_length_words\u0027, \u0027text\u0027]], y_encrypted)\n\nmetrics_encrypted, history, report_df \u003d apply_and_evaluate_deep_learning_model(X_encrypted, y_encrypted)\nprint(f\"Metrics on encrypted data:\")\nmetrics_encrypted.head()\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "scrolled": false
      },
      "source": [
        "report_df.head()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pd.DataFrame(history.history).plot()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    }
  ]
}