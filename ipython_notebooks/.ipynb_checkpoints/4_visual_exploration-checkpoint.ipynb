{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-venv-edf_sentiment_analysis",
      "display_name": "Python (env edf_sentiment_analysis)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.20",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "hide_input": false,
    "creator": "anne-soline.guilbert-ly@dataiku.com",
    "dkuGit": {
      "lastInteraction": 1742215088277,
      "gitReference": {
        "remote": "git@github.com:annesoline/sentiment-analysis.git",
        "checkout": "refs/heads/main",
        "remotePath": "visual_exploration.ipynb",
        "remoteLogin": "",
        "lastHash": "4dcb661cf1fe4521ac95b4dac17350d37f0d6261",
        "lastTimestamp": 1742215053000,
        "isDirty": false
      }
    },
    "createdOn": 1742215088267,
    "tags": [],
    "modifiedBy": "anne-soline.guilbert-ly@dataiku.com",
    "versionNumber": 1
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\nimport dataiku\nimport pandas as pd\n# Word Cloud Visualization\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport re\nimport nltk\nfrom nltk import pos_tag\nfrom nltk.tokenize import word_tokenize\nimport seaborn as sns\nfrom collections import Counter\n\npd.set_option(\u0027display.max_colwidth\u0027, None)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "enhanced_tweets_informations \u003d dataiku.Dataset(\"enhanced_tweets_informations\")\ndf \u003d enhanced_tweets_informations.get_dataframe()\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Visual exploration of textual data"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Filter the data randomly and keep 10000 rows\n# df \u003d df.sample(n\u003d10000, random_state\u003d42)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Combine all tweets into one text\nall_text \u003d \u0027 \u0027.join(df[\u0027text\u0027].astype(str))\n\n# Clean text - remove URLs, mentions, special chars\nall_text \u003d re.sub(r\u0027http\\S+|@\\S+|[^\\w\\s]\u0027, \u0027\u0027, all_text.lower())\n\n# Create word cloud\nwordcloud \u003d WordCloud(width\u003d1200, height\u003d600, \n                     background_color\u003d\u0027white\u0027,\n                     max_words\u003d100).generate(all_text)\n\n# Display the word cloud\nplt.figure(figsize\u003d(12, 6))\nplt.imshow(wordcloud, interpolation\u003d\u0027bilinear\u0027)\nplt.axis(\u0027off\u0027)\nplt.title(\u0027Most Frequent Words in Tweets\u0027)\nplt.show()\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Tokenize the cleaned text\ntokens \u003d word_tokenize(all_text)\n\n# Tag tokens with part of speech\ntagged_tokens \u003d pos_tag(tokens)\n\n# Filter for common nouns (NN, NNS)\ncommon_nouns \u003d [word for word, pos in tagged_tokens if pos in (\u0027NN\u0027, \u0027NNS\u0027)]\n\n# Count noun frequencies\nnoun_counts \u003d Counter(common_nouns)\n\n# Get the 20 most common nouns and their counts\ncommon_nouns \u003d noun_counts.most_common(20)\nnouns, noun_counts \u003d zip(*common_nouns)\n\n# Plot the bar chart for common nouns\nplt.figure(figsize\u003d(12, 6))\nplt.bar(nouns, noun_counts, color\u003d\u0027lightgreen\u0027)\nplt.xticks(rotation\u003d45, ha\u003d\u0027right\u0027)\nplt.title(\u0027Top 20 Most Frequent Common Nouns in Tweets\u0027)\nplt.xlabel(\u0027Nouns\u0027)\nplt.ylabel(\u0027Frequency\u0027)\nplt.tight_layout()\nplt.show()\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# Create a bigram frequency distribution\nbigrams \u003d list(nltk.bigrams(tokens))\nbigram_counts \u003d Counter(bigrams)\n\n# Get the 20 most common bigrams and their counts\ncommon_bigrams \u003d bigram_counts.most_common(20)\nbigrams, bigram_counts \u003d zip(*common_bigrams)\n\n# Convert bigrams to a more readable format\nbigrams \u003d [\u0027 \u0027.join(bigram) for bigram in bigrams]\n\n# Plot the bar chart for common bigrams\nplt.figure(figsize\u003d(12, 6))\nplt.bar(bigrams, bigram_counts, color\u003d\u0027skyblue\u0027)\nplt.xticks(rotation\u003d45, ha\u003d\u0027right\u0027)\nplt.title(\u0027Top 20 Most Frequent Bigrams in Tweets\u0027)\nplt.xlabel(\u0027Bigrams\u0027)\nplt.ylabel(\u0027Frequency\u0027)\nplt.tight_layout()\nplt.show()\n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a trigram frequency distribution\ntrigrams \u003d list(nltk.trigrams(tokens))\ntrigram_counts \u003d Counter(trigrams)\n\n# Get the 20 most common trigrams and their counts\ncommon_trigrams \u003d trigram_counts.most_common(20)\ntrigrams, trigram_counts \u003d zip(*common_trigrams)\n\n# Convert trigrams to a more readable format\ntrigrams \u003d [\u0027 \u0027.join(trigram) for trigram in trigrams]\n\n# Plot the bar chart for common trigrams\nplt.figure(figsize\u003d(12, 6))\nplt.bar(trigrams, trigram_counts, color\u003d\u0027salmon\u0027)\nplt.xticks(rotation\u003d45, ha\u003d\u0027right\u0027)\nplt.title(\u0027Top 20 Most Frequent Trigrams in Tweets\u0027)\nplt.xlabel(\u0027Trigrams\u0027)\nplt.ylabel(\u0027Frequency\u0027)\nplt.tight_layout()\nplt.show()\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure \u0027target\u0027 is a numerical column for correlation calculation\n# First, convert non-numeric \u0027target\u0027 values to NaN\ndf[\u0027target_numeric\u0027] \u003d pd.to_numeric(df[\u0027target\u0027], errors\u003d\u0027coerce\u0027)\n\n# Convert \u0027date\u0027 column to datetime to avoid ValueError during correlation calculation\ndf[\u0027date\u0027] \u003d pd.to_datetime(df[\u0027date\u0027], errors\u003d\u0027coerce\u0027)\n\n# Drop rows where \u0027target_numeric\u0027, \u0027date\u0027, or any other non-numeric column is NaN to avoid ValueError\ndf_clean \u003d df.dropna(subset\u003d[\u0027target_numeric\u0027, \u0027date\u0027])\n\n# Convert all non-numeric columns to numeric, coercing errors to NaN\nfor column in df_clean.select_dtypes(include\u003d[\u0027object\u0027]).columns:\n    df_clean[column] \u003d pd.to_numeric(df_clean[column], errors\u003d\u0027coerce\u0027)\n\n# Calculate the correlation between the target and other numerical columns, excluding \u0027flag\u0027 and \u0027id\u0027\ncorrelation_matrix \u003d df_clean.drop(columns\u003d[\u0027flag\u0027, \u0027id\u0027], errors\u003d\u0027ignore\u0027).corr()\n\n# Extract the correlation values for the \u0027target_numeric\u0027 column\ntarget_correlation \u003d correlation_matrix[\u0027target_numeric\u0027].drop(\u0027target_numeric\u0027)\n\n# Create another graph: Heatmap of the correlation matrix\nplt.figure(figsize\u003d(10, 8))\nsns.heatmap(correlation_matrix, annot\u003dTrue, cmap\u003d\u0027coolwarm\u0027, fmt\u003d\".2f\")\nplt.title(\u0027Heatmap of Correlation Matrix\u0027)\nplt.tight_layout()\nplt.show()\n"
      ],
      "outputs": []
    }
  ]
}