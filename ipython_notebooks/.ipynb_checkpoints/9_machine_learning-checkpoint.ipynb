{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-containerized-venv-edf_sentiment_analysis-cpu-xl-2-cpu-16gb-ram",
      "display_name": "Python in CPU-XL-2-cpu-16Gb-Ram (env edf_sentiment_analysis)",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.9.20",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "creator": "anne-soline.guilbert-ly@dataiku.com",
    "dkuGit": {
      "lastInteraction": 0
    },
    "customFields": {},
    "creationTag": {
      "versionNumber": 0,
      "lastModifiedBy": {
        "login": "anne-soline.guilbert-ly@dataiku.com"
      },
      "lastModifiedOn": 1742812779766
    },
    "createdOn": 1742812779766,
    "tags": [
      "recipe-editor"
    ],
    "associatedRecipe": "recipe_from_notebook_9_machine_learning",
    "modifiedBy": "anne-soline.guilbert-ly@dataiku.com"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nfrom dataiku import pandasutils as pdu\nimport pandas as pd\nfrom datetime import datetime\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "prepared_tweets_encryption \u003d dataiku.Dataset(\"prepared_tweets_encryption\")\nprepared_tweets_encryption_df \u003d prepared_tweets_encryption.get_dataframe()\n\nprepared_tweets_removal \u003d dataiku.Dataset(\"prepared_tweets_removal\")\nprepared_tweets_removal_df \u003d prepared_tweets_removal.get_dataframe()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, roc_auc_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\ndef apply_and_evaluate_model_with_tfidf_stratified_kfold(df: pd.DataFrame, label_col: str \u003d \u0027label\u0027, n_splits: int \u003d 5, max_features: int \u003d 5000):\n    \"\"\"\n    Applies a logistic regression model using TF-IDF features and evaluates it with stratified K-fold cross-validation.\n\n    Parameters:\n    df (pd.DataFrame): The input DataFrame containing the dataset.\n    label_col (str): The name of the column containing the target labels. Default is \u0027label\u0027.\n    n_splits (int): The number of splits for stratified K-fold cross-validation. Default is 5.\n    max_features (int): The maximum number of features to consider for the TF-IDF vectorizer. Default is 5000.\n\n    Returns:\n    tuple: A tuple containing the list of accuracies, classification reports, and ROC AUC scores for each fold.\n    \"\"\"\n\n    # Step 2: Split dataset into features and target\n    # Handle date, user, and language as dummy variables\n    X \u003d pd.get_dummies(df[[\u0027date\u0027, \u0027user\u0027, \u0027language\u0027]], drop_first\u003dTrue)\n\n    # Handle numerical columns with standard scaling\n    numerical_cols \u003d [\u0027tweet_length_chars\u0027, \u0027tweet_length_words\u0027, \u0027repetitive_letters\u0027,\n                      \u0027mention_only\u0027, \u0027unreadable\u0027, \u0027too_many_numbers\u0027]\n    scaler \u003d StandardScaler()\n    X_numerical \u003d scaler.fit_transform(df[numerical_cols].fillna(0))\n    X_numerical \u003d pd.DataFrame(X_numerical, columns\u003dnumerical_cols, index\u003ddf.index)\n\n    # Concatenate all features\n    X \u003d pd.concat([X, X_numerical], axis\u003d1)\n    y \u003d df[label_col]\n\n    # Step 3: Apply TF-IDF transformation on the text column\n    tfidf \u003d TfidfVectorizer(\n        min_df\u003d0.01,  # Adjusted min_df to a lower value\n        max_df\u003d0.9,   # Adjusted max_df to a higher value\n        ngram_range\u003d(1, 1),\n        stop_words\u003dNone\n    )\n\n    try:\n        X_tfidf \u003d tfidf.fit_transform(df[\u0027text\u0027].fillna(\u0027\u0027))\n    except ValueError as e:\n        print(f\"Error during TF-IDF transformation: {e}\")\n        return None, None, None\n\n    # Combine TF-IDF features with other features\n    X_combined \u003d np.hstack((X.values, X_tfidf.toarray()))\n\n    # Step 4: Stratified K-Fold Cross Validation\n    skf \u003d StratifiedKFold(n_splits\u003dn_splits, shuffle\u003dTrue, random_state\u003d42)\n    accuracies \u003d []\n    reports \u003d []\n    roc_aucs \u003d []\n\n    for train_index, test_index in skf.split(X_combined, y):\n        X_train, X_test \u003d X_combined[train_index], X_combined[test_index]\n        y_train, y_test \u003d y.iloc[train_index], y.iloc[test_index]\n\n        # Define the model\n        model \u003d LogisticRegression(max_iter\u003d1000)\n\n        # Train the model\n        model.fit(X_train, y_train)\n\n        # Predict on the test set\n        y_pred \u003d model.predict(X_test)\n        y_pred_proba \u003d model.predict_proba(X_test)\n\n        # Evaluate the model\n        accuracy \u003d accuracy_score(y_test, y_pred)\n        report \u003d classification_report(y_test, y_pred, output_dict\u003dTrue)\n\n        try:\n            roc_auc \u003d roc_auc_score(y_test, y_pred_proba, multi_class\u003d\u0027ovr\u0027)\n        except ValueError as e:\n            print(f\"Error during ROC AUC calculation: {e}\")\n            roc_auc \u003d None\n\n        accuracies.append(accuracy)\n        reports.append(report)\n        roc_aucs.append(roc_auc)\n\n    return np.mean(accuracies), reports, np.mean([auc for auc in roc_aucs if auc is not None]), model"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train and evaluate prepared_tweets_encryption_df"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Apply the function to each DataFrame\naccuracy_encrypted, report_encrypted, roc_auc_encrypted, model_encrypted_data \u003d apply_and_evaluate_model_with_tfidf_stratified_kfold(prepared_tweets_encryption_df)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if accuracy_encrypted is not None:\n    encrypted_metrics_df \u003d pd.DataFrame({\n        \u0027Metric\u0027: [\u0027Average Accuracy\u0027, \u0027Average ROC AUC\u0027],\n        \u0027Value\u0027: [accuracy_encrypted, roc_auc_encrypted]\n    })\n    print(encrypted_metrics_df)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "encrypted_metrics_per_fold_df \u003d pd.DataFrame()\n\nfor i, report in enumerate(report_encrypted):\n    if isinstance(report, dict):\n        # Remove \u0027accuracy\u0027, \u0027macro avg\u0027, and \u0027weighted avg\u0027 from the report\n        report.pop(\u0027accuracy\u0027, None)\n        report.pop(\u0027macro avg\u0027, None)\n        report.pop(\u0027weighted avg\u0027, None)\n\n        report_df \u003d pd.DataFrame.from_dict(report).transpose()\n        report_df[\u0027class\u0027] \u003d report_df.index  # Save the key of each dictionary into a new column called \"class\"\n        report_df[\u0027Fold\u0027] \u003d i + 1  # Add the fold number to the DataFrame\n        encrypted_metrics_per_fold_df \u003d pd.concat([encrypted_metrics_per_fold_df, report_df], ignore_index\u003dTrue)\n    else:\n        print(f\"Warning: Report for Fold {i+1} is not a dictionary and cannot be converted to a DataFrame.\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "artefact_name_encrypted \u003d f\"lr_{datetime.now().strftime(\u0027%Y-%m-%d_%H-%M-%S\u0027)}\"\nencrypted_metrics_per_fold_df[\"model\"] \u003d artefact_name_encrypted"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train and evaluate to prepared_tweets_removal_df"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "accuracy_removed, report_removed, roc_auc_removed, model_removed_data \u003d apply_and_evaluate_model_with_tfidf_stratified_kfold(prepared_tweets_removal_df)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if accuracy_removed is not None:\n    removed_metrics_df \u003d pd.DataFrame({\n        \u0027Metric\u0027: [\u0027Average Accuracy\u0027, \u0027Average ROC AUC\u0027],\n        \u0027Value\u0027: [accuracy_removed, roc_auc_removed]\n    })\n    print(removed_metrics_df)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "removed_metrics_per_fold_df \u003d pd.DataFrame()\n\nfor i, report in enumerate(report_removed):\n    if isinstance(report, dict):\n        # Remove \u0027accuracy\u0027, \u0027macro avg\u0027, and \u0027weighted avg\u0027 from the report\n        report.pop(\u0027accuracy\u0027, None)\n        report.pop(\u0027macro avg\u0027, None)\n        report.pop(\u0027weighted avg\u0027, None)\n\n        report_df \u003d pd.DataFrame.from_dict(report).transpose()\n        report_df[\u0027class\u0027] \u003d report_df.index  # Save the key of each dictionary into a new column called \"class\"\n        report_df[\u0027Fold\u0027] \u003d i + 1  # Add the fold number to the DataFrame\n        removed_metrics_per_fold_df \u003d pd.concat([removed_metrics_per_fold_df, report_df], ignore_index\u003dTrue)\n    else:\n        print(f\"Warning: Report for Fold {i+1} is not a dictionary and cannot be converted to a DataFrame.\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "artefact_name_removed \u003d f\"lr_{datetime.now().strftime(\u0027%Y-%m-%d_%H-%M-%S\u0027)}\"\nremoved_metrics_per_fold_df[\"model\"] \u003d artefact_name_removed"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create output datasets"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Recipe outputs\nencrypted_metrics \u003d dataiku.Dataset(\"encrypted_metrics\")\nencrypted_metrics.write_with_schema(encrypted_metrics_df)\n\nencrypted_metrics_per_fold \u003d dataiku.Dataset(\"encrypted_metrics_per_fold\")\nencrypted_metrics_per_fold.write_with_schema(encrypted_metrics_per_fold_df)\n\nremoved_metrics \u003d dataiku.Dataset(\"removed_metrics\")\nremoved_metrics.write_with_schema(removed_metrics_df)\n\nremoved_metrics_per_fold \u003d dataiku.Dataset(\"removed_metrics_per_fold\")\nremoved_metrics_per_fold.write_with_schema(removed_metrics_per_fold_df)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Outputs"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Pipeline\npipeline_local_path_encrypted \u003d f\"{artefact_name_encrypted}.pkl\"\npipeline_remote_path_encrypted \u003d f\"{artefact_name_encrypted}.pkl\"\nremote_output_folder_encrypted \u003d dataiku.Folder(\"VQ6fLov2\")\n\nwith tempfile.TemporaryDirectory() as local_tmp_dir_encrypted:\n\n    local_file_path_encrypted \u003d os.path.join(local_tmp_dir_encrypted, pipeline_local_path_encrypted)\n\n    with open(local_file_path_encrypted, \u0027wb\u0027) as file:\n        pickle.dump(model_encrypted_data, file)\n\n    remote_output_folder_encrypted.upload_file(pipeline_remote_path_encrypted, local_file_path_encrypted)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Pipeline for removed\npipeline_local_path_removed \u003d f\"{artefact_name_removed}.pkl\"\npipeline_remote_path_removed \u003d f\"{artefact_name_removed}.pkl\"\nremote_output_folder_removed \u003d dataiku.Folder(\"VQ6fLov2\")\n\nwith tempfile.TemporaryDirectory() as local_tmp_dir_removed:\n\n    local_file_path_removed \u003d os.path.join(local_tmp_dir_removed, pipeline_local_path_removed)\n\n    with open(local_file_path_removed, \u0027wb\u0027) as file:\n        pickle.dump(model_removed_data, file)\n\n    remote_output_folder_removed.upload_file(pipeline_remote_path_removed, local_file_path_removed)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Artefacts\nfi_local_path \u003d f\"{artefact_name}_feature_importance.png\"\nfi_remote_path \u003d f\"{artefact_name}_feature_importance.png\"\noutput_folder \u003d dataiku.Folder(\"2T1uAdOy\")\n\nwith tempfile.TemporaryDirectory() as tmp_dir_name:\n    local_file_path \u003d os.path.join(tmp_dir_name, fi_local_path)\n    fig \u003d ax.get_figure()\n    fig.savefig(fi_local_path)\n    output_folder.upload_file(fi_remote_path, fi_local_path)\n    plt.close(fig)"
      ],
      "outputs": []
    }
  ]
}