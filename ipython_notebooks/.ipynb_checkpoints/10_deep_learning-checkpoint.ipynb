{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python in GPU-S-1-GPU (env edf_sentiment_analysis)",
      "language": "python",
      "name": "py-dku-containerized-venv-edf_sentiment_analysis-gpu-s-1-gpu"
    },
    "associatedRecipe": "10_deep_learning",
    "dkuGit": {
      "lastInteraction": 0
    },
    "creationTag": {
      "versionNumber": 0,
      "lastModifiedBy": {
        "login": "anne-soline.guilbert-ly@dataiku.com"
      },
      "lastModifiedOn": 1743093624999
    },
    "creator": "anne-soline.guilbert-ly@dataiku.com",
    "createdOn": 1743093624999,
    "tags": [
      "recipe-editor"
    ],
    "customFields": {}
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "import dataiku\n",
        "from dataiku import pandasutils as pdu\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datetime import datetime\n",
        "import os\n",
        "import tempfile\n",
        "import pickle\n",
        "\n",
        "from edf_commons.modelling import preprocess_data_for_dl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "LABEL_MAPPING \u003d {\u0027very negative\u0027: 0, \u0027negative\u0027: 1, \u0027neutral\u0027: 2, \u0027positive\u0027: 3, \u0027very positive\u0027: 4}\n",
        "INDEX_MAPPING \u003d {v: k for k, v in LABEL_MAPPING.items()}\n",
        "DATE_TIME \u003d datetime.now().strftime(\u0027%Y-%m-%d_%H-%M-%S\u0027)\n",
        "\n",
        "# Global variables\n",
        "project \u003d dataiku.Project()\n",
        "variables \u003d project.get_variables()\n",
        "ARTEFACTS_FOLDER_ID \u003d variables[\"standard\"][\"artefacts_folder_id\"]\n",
        "CONFUSION_MATRICES_FOLDER_ID \u003d variables[\"standard\"][\"confusion_matrices_path\"]\n",
        "EPOCHS_PERF_FOLDER_ID \u003d variables[\"standard\"][\"epochs_perf_path\"]\n",
        "DL_MODELS_FOLDER_ID \u003d variables[\"standard\"][\"df_model_folder_id\"]\n",
        "DL_MODELS_DATA_FOLDER \u003d dataiku.Folder(DL_MODELS_FOLDER_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "tweets_train \u003d dataiku.Dataset(\"tweets_train\")\n",
        "df \u003d tweets_train.get_dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# Preprocess the data\n",
        "y \u003d df[\u0027label\u0027]\n",
        "X, y \u003d preprocess_data_for_dl(df[[\u0027tweet_length_chars\u0027, \u0027tweet_length_words\u0027, \u0027text\u0027]], y)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "def save_image(img_name: str, folder_path: str)-\u003eNone:\n",
        "    \"\"\"Save the image of a graph recently plotted.\n",
        "\n",
        "    ----------\n",
        "    Parameters\n",
        "        img_name: str\n",
        "            Name of the image given to the png file.\n",
        "        folder_path: str\n",
        "            Path to the sub folder within the managed folder.\n",
        "    \"\"\"\n",
        "\n",
        "    # Artefacts\n",
        "    fig_name \u003d f\"{img_name}_{DATE_TIME}.png\"\n",
        "    output_folder \u003d dataiku.Folder(ARTEFACTS_FOLDER_ID)\n",
        "    output_folder_path \u003d os.path.join(folder_path, fig_name)\n",
        "\n",
        "    with tempfile.TemporaryDirectory() as tmp_dir_name:\n",
        "        local_file_path \u003d os.path.join(tmp_dir_name, fig_name)\n",
        "        plt.savefig(fig_name)\n",
        "        output_folder.upload_file(output_folder_path, fig_name)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "def apply_and_evaluate_deep_learning_model(X: pd.DataFrame, y: pd.Series) -\u003e tuple[pd.DataFrame, tf.keras.callbacks.History, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Applies a deep learning model to the preprocessed data and evaluates its performance.\n",
        "\n",
        "    Parameters:\n",
        "    X (pd.DataFrame): The preprocessed data including numerical, categorical, and text data.\n",
        "    y (pd.Series): The labels.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: A DataFrame containing the loss and accuracy of the model on the test data.\n",
        "    \"\"\"\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test \u003d train_test_split(X, y, test_size\u003d0.2, random_state\u003d42)\n",
        "    num_classes \u003d len(y.unique())\n",
        "\n",
        "    # Convert processed data to TensorFlow datasets\n",
        "    train_dataset \u003d tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values))\n",
        "    test_dataset \u003d tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values))\n",
        "\n",
        "    # Batch the datasets\n",
        "    train_dataset \u003d train_dataset.batch(32)\n",
        "    test_dataset \u003d test_dataset.batch(32)\n",
        "\n",
        "    # Initialize the model\n",
        "    model \u003d Sequential([\n",
        "        Embedding(input_dim\u003d10000, output_dim\u003d128),\n",
        "        LSTM(64, return_sequences\u003dTrue),\n",
        "        Dropout(0.5),\n",
        "        LSTM(64),\n",
        "        Dense(32, activation\u003d\u0027relu\u0027),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation\u003d\u0027softmax\u0027)  # Sigmoid for binary classification (positive/negative sentiment)\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer\u003dAdam(learning_rate\u003d0.00001),\n",
        "                  loss\u003d\u0027sparse_categorical_crossentropy\u0027,\n",
        "                  metrics\u003d[\u0027accuracy\u0027])\n",
        "\n",
        "    # Train the model\n",
        "    history \u003d model.fit(train_dataset, epochs\u003d10, validation_data\u003dtest_dataset, verbose\u003d1)\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy \u003d model.evaluate(test_dataset, verbose\u003d0)\n",
        "\n",
        "    # Obtenir les prédictions\n",
        "    y_pred \u003d model.predict(X_test.values)\n",
        "    y_pred_classes \u003d y_pred.argmax(axis\u003d1)\n",
        "\n",
        "    # Afficher la matrice\n",
        "    cm \u003d confusion_matrix(y_test, y_pred_classes)\n",
        "    disp \u003d ConfusionMatrixDisplay(confusion_matrix\u003dcm)\n",
        "    disp.plot()\n",
        "\n",
        "    # Save the image\n",
        "    save_image(\"confusion_matrix\", CONFUSION_MATRICES_FOLDER_ID)\n",
        "\n",
        "    # Générer le rapport\n",
        "    report_dict \u003d classification_report(y_test, y_pred_classes, output_dict\u003dTrue, target_names\u003dy_test.unique())\n",
        "    print(report_dict)\n",
        "\n",
        "    # Le convertir en DataFrame\n",
        "    report_df \u003d pd.DataFrame(report_dict).transpose()\n",
        "\n",
        "    # Save metrics into a DataFrame\n",
        "    metrics_df \u003d pd.DataFrame({\u0027average_loss\u0027: [loss], \u0027average_accuracy\u0027: [accuracy]})\n",
        "\n",
        "    return metrics_df, history, report_df, model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# Evaluate model on encrypted data\n",
        "metrics, history, report_df, dl_model \u003d apply_and_evaluate_deep_learning_model(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save pickle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# Pipeline for encrypted data\n",
        "nn_artefact_name \u003d f\"nn_{DATE_TIME}\"\n",
        "artefact_pickle_name \u003d f\"{nn_artefact_name}.pkl\"\n",
        "\n",
        "with tempfile.TemporaryDirectory() as temp_dir:\n",
        "\n",
        "    local_file_path \u003d os.path.join(temp_dir, artefact_pickle_name)\n",
        "\n",
        "    with open(local_file_path, \u0027wb\u0027) as file:\n",
        "        pickle.dump(dl_model, file)\n",
        "\n",
        "    DL_MODELS_DATA_FOLDER.upload_file(artefact_pickle_name, local_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# remove last metrics\n",
        "report_df \u003d report_df.iloc[:-3]\n",
        "# add model name column\n",
        "report_df[\"model\"] \u003d nn_artefact_name\n",
        "# add date time column\n",
        "report_df[\"date_time\"] \u003d DATE_TIME\n",
        "# create label column by remapping the names\n",
        "report_df[\u0027label\u0027] \u003d report_df.index.map(INDEX_MAPPING)\n",
        "for col in metrics.columns:\n",
        "    report_df[col] \u003d metrics[col].iloc[0]\n",
        "\n",
        "# drop the indexes\n",
        "report_df.reset_index(drop\u003dTrue, inplace\u003dTrue)\n",
        "\n",
        "# Recipe outputs\n",
        "dl_metrics \u003d dataiku.Dataset(\"dl_metrics\")\n",
        "dl_metrics.write_with_schema(report_df)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "plot \u003d pd.DataFrame(history.history).plot()\n",
        "fig \u003d plot.get_figure()\n",
        "name \u003d f\"epochs_perf_evolution_{DATE_TIME}.png\"\n",
        "save_image(\"epochs_perf_evolution_\", EPOCHS_PERF_FOLDER_ID)"
      ]
    }
  ]
}