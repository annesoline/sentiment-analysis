{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python in CPU-XL-2-cpu-16Gb-Ram (env edf_sentiment_analysis)",
      "language": "python",
      "name": "py-dku-containerized-venv-edf_sentiment_analysis-cpu-xl-2-cpu-16gb-ram"
    },
    "associatedRecipe": "11_1_evaluation_machine_learning",
    "dkuGit": {
      "lastInteraction": 0
    },
    "creationTag": {
      "versionNumber": 0,
      "lastModifiedBy": {
        "login": "anne-soline.guilbert-ly@dataiku.com"
      },
      "lastModifiedOn": 1743093283077
    },
    "creator": "anne-soline.guilbert-ly@dataiku.com",
    "createdOn": 1743093283077,
    "tags": [
      "recipe-editor"
    ],
    "customFields": {}
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "import dataiku\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "from edf_commons.machine_learning import preprocess_data\n",
        "\n",
        "import tempfile\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "import pickle\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "project \u003d dataiku.Project()\n",
        "variables \u003d project.get_variables()\n",
        "MODELS_PATH \u003d variables[\"standard\"][\"models_path\"]\n",
        "MODEL_FOLDER_ID \u003d variables[\"standard\"][\"model_folder_id\"]\n",
        "MODELS_DATA_FOLDER \u003d dataiku.Folder(MODEL_FOLDER_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "tweets_eval \u003d dataiku.Dataset(\"prepared_tweets\")\n",
        "eval_df \u003d tweets_eval.get_dataframe()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# Get the TFIDF model\n",
        "tfidf_path \u003d [path for path in MODELS_DATA_FOLDER.list_paths_in_partition() if \"/tfidf/\" in path][-1]\n",
        "tfidf_name \u003d tfidf_path.split(\"/\")[-1].split(\".pkl\")[0]\n",
        "\n",
        "# load latest TFIDF model\n",
        "with tempfile.TemporaryDirectory() as temp_directory_name:\n",
        "\n",
        "    local_file_path \u003d temp_directory_name + \"/\" + tfidf_name\n",
        "\n",
        "    # Copy file from remote to local\n",
        "    with MODELS_DATA_FOLDER.get_download_stream(tfidf_path) as f_remote, open(local_file_path,\u0027wb\u0027) as f_local:\n",
        "        shutil.copyfileobj(f_remote, f_local)\n",
        "\n",
        "    # Load the pipeline\n",
        "    with open(local_file_path, \u0027rb\u0027) as file:\n",
        "        tfidf \u003d pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# Get the Logistic Regression model\n",
        "lr_path \u003d [path for path in MODELS_DATA_FOLDER.list_paths_in_partition() if \"/lr/\" in path][-1]\n",
        "lr_name \u003d model_path.split(\"/\")[-1].split(\".pkl\")[0]\n",
        "\n",
        "# load latest TFIDF model\n",
        "with tempfile.TemporaryDirectory() as temp_directory_name:\n",
        "\n",
        "    local_file_path \u003d temp_directory_name + \"/\" + lr_name\n",
        "\n",
        "    # Copy file from remote to local\n",
        "    with MODELS_DATA_FOLDER.get_download_stream(lr_path) as f_remote, open(local_file_path,\u0027wb\u0027) as f_local:\n",
        "        shutil.copyfileobj(f_remote, f_local)\n",
        "\n",
        "    # Load the pipeline\n",
        "    with open(local_file_path, \u0027rb\u0027) as file:\n",
        "        lr_model \u003d pickle.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# Preprocess the data\n",
        "X_eval, y_eval, _ \u003d preprocess_data(eval_df, tfidf)\n",
        "\n",
        "# Predict\n",
        "eval_prediction_results_df \u003d eval_df.copy()\n",
        "eval_prediction_results_df[\u0027predicted\u0027] \u003d lr_model.predict(X_eval)\n",
        "eval_prediction_results_df[\u0027probability\u0027] \u003d lr_model.predict_proba(X_eval)[:, 1]\n",
        "eval_prediction_results_df[\u0027correct_prediction\u0027] \u003d eval_prediction_results_df[\u0027label\u0027] \u003d\u003d eval_prediction_results_df[\u0027predicted\u0027]\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy \u003d accuracy_score(y_eval, eval_prediction_results_df[\u0027predicted\u0027])\n",
        "precision \u003d precision_score(y_eval, eval_prediction_results_df[\u0027predicted\u0027], average\u003d\u0027weighted\u0027)\n",
        "recall \u003d recall_score(y_eval, eval_prediction_results_df[\u0027predicted\u0027], average\u003d\u0027weighted\u0027)\n",
        "f1 \u003d f1_score(y_eval, eval_prediction_results_df[\u0027predicted\u0027], average\u003d\u0027weighted\u0027)\n",
        "roc_auc \u003d roc_auc_score(y_eval, lr_model.predict_proba(X_eval), multi_class\u003d\u0027ovr\u0027, average\u003d\u0027weighted\u0027)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"ROC AUC: {roc_auc}\")\n",
        "\n",
        "eval_metrics_df \u003d pd.DataFrame({\n",
        "    \u0027Mean Accuracy\u0027: [accuracy],\n",
        "    \u0027Mean Precision\u0027: [precision],\n",
        "    \u0027Mean Recall\u0027: [recall],\n",
        "    \u0027Mean F1 Score\u0027: [f1],\n",
        "    \u0027Mean ROC AUC\u0027: [roc_auc],\n",
        "    \u0027Model Name\u0027: [\u0027Logistic Regression\u0027],\n",
        "    \u0027Date and Time\u0027: [datetime.now().strftime(\u0027%Y-%m-%d %H:%M:%S\u0027)]\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# Recipe outputs\n",
        "eval_prediction_results \u003d dataiku.Dataset(\"eval_prediction_results\")\n",
        "eval_prediction_results.write_with_schema(eval_prediction_results_df)\n",
        "\n",
        "eval_metrics \u003d dataiku.Dataset(\"eval_metrics\")\n",
        "eval_metrics.write_with_schema(eval_metrics_df)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# Recipe outputs\n",
        "eval_prediction_results \u003d dataiku.Dataset(\"eval_prediction_results\")\n",
        "eval_prediction_results.write_with_schema(pandas_dataframe)\n",
        "eval_metrics \u003d dataiku.Dataset(\"eval_metrics\")\n",
        "eval_metrics.write_with_schema(pandas_dataframe)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# Recipe outputs\n",
        "eval_metrics \u003d dataiku.Dataset(\"eval_metrics\")\n",
        "eval_metrics.write_with_schema(pandas_dataframe)\n",
        "eval_prediction_results \u003d dataiku.Dataset(\"eval_prediction_results\")\n",
        "eval_prediction_results.write_with_schema(pandas_dataframe)"
      ]
    }
  ]
}