{
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "creator": "anne-soline.guilbert-ly@dataiku.com",
    "createdOn": 1741971467381,
    "tags": [],
    "customFields": {}
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "pd.set_option(\u0027display.max_colwidth\u0027, None)\n",
        "\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Load tweets dataset"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df \u003d pd.read_csv(\u0027training.1600000.processed.noemoticon.csv\u0027, encoding\u003d\u0027latin-1\u0027, names\u003d[\u0027target\u0027, \u0027id\u0027, \u0027date\u0027, \u0027flag\u0027, \u0027user\u0027, \u0027text\u0027])"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df.head()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Textual data quality analysis\n",
        "## 2.1. Basic information"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df.info()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Values taken by the column flag: {df[\u0027flag\u0027].unique()[0]}\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"There are {df[\u0027user\u0027].nunique()} different users.\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check for empty tweets\n",
        "empty_tweets \u003d len(df[df[\u0027text\u0027].str.len() \u003d\u003d 0])\n",
        "print(f\"\\nNumber of empty tweets: {empty_tweets}\")\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2. Tweet length"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df[\u0027tweet_length_chars\u0027] \u003d df[\u0027text\u0027].str.len()\n",
        "df[\u0027tweet_length_words\u0027] \u003d df[\u0027text\u0027].str.split().apply(len)\n",
        "df.head()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Tweet column analysis\n",
        "print(\"\\nTweet length statistics:\")\n",
        "print(df[\u0027tweet_length_chars\u0027].describe())\n",
        "\n",
        "# Plot distribution of tweet lengths\n",
        "plt.figure(figsize\u003d(12,6))\n",
        "plt.hist(df[\u0027tweet_length_chars\u0027], bins\u003d50, edgecolor\u003d\u0027black\u0027)\n",
        "plt.title(\u0027Distribution of Tweet Lengths\u0027)\n",
        "plt.xlabel(\u0027Number of Characters\u0027)\n",
        "plt.ylabel(\u0027Frequency\u0027)\n",
        "plt.show()\n",
        "\n",
        "# Most common tweet lengths in words\n",
        "print(\"\\nMost common tweet lengths (in words):\")\n",
        "print(df[\u0027tweet_length_words\u0027].value_counts().head())"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3. Tweet specificities (characters, URL, and mentions)"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Unique characters analysis\n",
        "all_chars \u003d \u0027\u0027.join(df[\u0027text\u0027].values)\n",
        "unique_chars \u003d set(all_chars)\n",
        "print(f\"\\nNumber of unique characters used: {len(unique_chars)}\")\n",
        "print(f\"Unique characters used: {\u0027\u0027.join(sorted(unique_chars))}\")\n",
        "\n",
        "# Check for repetitive characters (like \u0027aaaaaa\u0027 or \u0027!!!!!!!\u0027)\n",
        "repetitive_chars \u003d df[df[\u0027text\u0027].str.match(r\u0027.*(.)\\1{4,}.*\u0027)].shape[0]\n",
        "print(f\"\\nTweets with repetitive characters: {repetitive_chars} ({(repetitive_chars/len(df)*100):.2f}%)\")\n",
        "print(\"\\nExamples of tweets with repetitive characters:\")\n",
        "print(df[df[\u0027text\u0027].str.match(r\u0027.*(.)\\1{4,}.*\u0027)][\u0027text\u0027].head(10))\n",
        "\n",
        "\n",
        "# URL and mention analysis\n",
        "tweets_with_urls \u003d len(df[df[\u0027text\u0027].str.contains(\u0027http|www\u0027, regex\u003dTrue)])\n",
        "tweets_with_mentions \u003d len(df[df[\u0027text\u0027].str.contains(\u0027@\u0027)])\n",
        "\n",
        "# Print examples of tweets with URLs\n",
        "print(\"\\nExample tweets containing URLs:\")\n",
        "urls \u003d df[\u0027text\u0027].str.extract(r\u0027\\b(http|www\\S+)\u0027, expand\u003dFalse)\n",
        "print(df[urls.notna()][\u0027text\u0027].head())\n",
        "\n",
        "# Print examples of tweets with mentions\n",
        "print(\"\\nExample tweets containing @mentions:\")\n",
        "print(df[df[\u0027text\u0027].str.contains(\u0027@\u0027)][\u0027text\u0027].head())\n",
        "\n",
        "print(f\"\\nTweets containing URLs: {tweets_with_urls} ({tweets_with_urls/len(df)*100:.2f}%)\")\n",
        "print(f\"Tweets containing @mentions: {tweets_with_mentions} ({tweets_with_mentions/len(df)*100:.2f}%)\")\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comments\n",
        "Tweets with repetitive characters often contain repeated dots or letters. For the latter, we can easily eliminate the repetition in the letters, helping the model better understand the words.\n",
        "\n",
        "It will be useful to remove all the URL from the text to ease the detection of sentiment in the text.\n",
        "\n",
        "Tweets containing only a mention in the text are tagged, allowing them to be removed from the dataset later, as they provide no relevant information for sentiment analysis."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create mention_only column\n",
        "# Pattern matches tweets that only contain @ followed by word characters\n",
        "df[\u0027mention_only\u0027] \u003d df[\u0027text\u0027].str.match(r\u0027^\\s*@\\w+\\s*$\u0027).astype(int)\n",
        "\n",
        "# Print summary\n",
        "print(f\"\\nTweets that are only mentions: {df[\u0027mention_only\u0027].sum()} ({df[\u0027mention_only\u0027].sum()/len(df)*100:.2f}%)\")\n",
        "print(\"\\nExample tweets that are only mentions:\")\n",
        "print(df[df[\u0027mention_only\u0027] \u003d\u003d 1][\u0027text\u0027].head())\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4. Tweets with special characters and unreadable tweets"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import string\n",
        "\n",
        "special_chars \u003d [c for c in unique_chars \n",
        "                if c not in string.ascii_letters \n",
        "                and c not in string.digits\n",
        "                and c not in string.punctuation\n",
        "                and not c.isalpha()\n",
        "                and c not in [\u0027¸\u0027, \u0027·\u0027, \u0027 \u0027, \u0027´\u0027, \u0027»\u0027, \u0027«\u0027]]  # Excludes accented letters, euro symbol, and specific characters\n",
        "print(\u0027Special characters:\u0027, (sorted(special_chars)))"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create unreadable column based on special character count\n",
        "df[\u0027unreadable\u0027] \u003d df[\u0027text\u0027].apply(lambda x: 1 if sum(1 for c in x if c in special_chars) \u003e 5 else 0)\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"\\nTweets with more than 5 special characters:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"\\nNumber of unreadable tweets: {df[\u0027unreadable\u0027].sum()}\")\n",
        "print(\"\\nExample unreadable tweets:\")\n",
        "print(df[df[\u0027unreadable\u0027] \u003d\u003d 1][[\u0027text\u0027]].head(10))\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Show tweets containing special characters\n",
        "print(\"\\nTweets containing special characters:\")\n",
        "print(\"-\" * 50)\n",
        "for char in special_chars:\n",
        "    tweets_with_char \u003d df[df[\u0027text\u0027].str.contains(char, regex\u003dFalse)]\n",
        "    if len(tweets_with_char) \u003e 0:\n",
        "        print(f\"\\nTweets containing \u0027{char}\u0027:\")\n",
        "        print(tweets_with_char[[\u0027text\u0027]].head())\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check for tweets with high percentage of numbers\n",
        "number_ratio \u003d df[\u0027text\u0027].str.count(r\u0027[0-9]\u0027) / df[\u0027tweet_length_chars\u0027]\n",
        "df[\u0027too_many_numbers\u0027] \u003d (number_ratio \u003e 0.3).astype(int)\n",
        "high_numbers \u003d df[\u0027too_many_numbers\u0027].sum()\n",
        "print(f\"\\nTweets with high number ratio (\u003e30%): {high_numbers} ({(high_numbers/len(df)*100):.2f}%)\")\n",
        "print(\"\\nExamples of tweets with many numbers:\")\n",
        "print(df[df[\u0027too_many_numbers\u0027] \u003d\u003d 1][\u0027text\u0027].head(10))\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comments\n",
        "In this section, we reviewed tweets containing excessive special characters that render them unreadable, and created a column to tag these tweets so they can be removed from the dataset later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.5. Short tweets, repetitive characters, all caps tweets"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check for very short tweets that might be low quality\n",
        "very_short_tweets \u003d df[df[\u0027tweet_length_chars\u0027] \u003c 10].shape[0]\n",
        "print(f\"\\nVery short tweets (\u003c10 chars): {very_short_tweets} ({(very_short_tweets/len(df)*100):.2f}%)\")\n",
        "print(\"\\nExamples of very short tweets:\")\n",
        "print(df[df[\u0027tweet_length_chars\u0027] \u003c 10][\u0027text\u0027].head(10))\n",
        "\n",
        "# Check for all caps tweets (possible spam/low quality)\n",
        "all_caps_tweets \u003d df[df[\u0027text\u0027].str.match(r\u0027^[A-Z0-9\\s\\W]+$\u0027)].shape[0]\n",
        "print(f\"\\nAll caps tweets: {all_caps_tweets} ({(all_caps_tweets/len(df)*100):.2f}%)\")\n",
        "print(\"\\nExamples of all caps tweets:\")\n",
        "print(df[df[\u0027text\u0027].str.match(r\u0027^[A-Z0-9\\s\\W]+$\u0027)][\u0027text\u0027].head(10))\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comments\n",
        "As shown in the example above, very short tweets can still be used for sentiment analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.6. Average punctuation marks per tweet, word/character ratio"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate average punctuation per tweet\n",
        "punct_counts \u003d df[\u0027text\u0027].str.count(f\u0027[{string.punctuation}]\u0027)\n",
        "avg_punct \u003d punct_counts.mean()\n",
        "print(f\"\\nAverage punctuation marks per tweet: {avg_punct:.2f}\")\n",
        "\n",
        "# Check for tweets with excessive punctuation\n",
        "excessive_punct \u003d df[punct_counts \u003e punct_counts.mean() + 2*punct_counts.std()].shape[0]\n",
        "print(f\"Tweets with excessive punctuation: {excessive_punct} ({(excessive_punct/len(df)*100):.2f}%)\")\n",
        "\n",
        "# Analyze word/character ratio (very low ratio might indicate spam or low quality)\n",
        "char_word_ratio \u003d df[\u0027tweet_length_chars\u0027] / df[\u0027tweet_length_words\u0027]\n",
        "suspicious_ratio \u003d df[char_word_ratio \u003e char_word_ratio.mean() + 2*char_word_ratio.std()].shape[0]\n",
        "print(f\"\\nTweets with suspicious character-to-word ratio: {suspicious_ratio} ({(suspicious_ratio/len(df)*100):.2f}%)\")\n",
        "print(\"\\nExamples of tweets with suspicious character-to-word ratio:\")\n",
        "print(df[char_word_ratio \u003e char_word_ratio.mean() + 2*char_word_ratio.std()][\u0027text\u0027].head(10))\n",
        "\n",
        "# Distribution of character-to-word ratios\n",
        "plt.figure(figsize\u003d(12, 6))\n",
        "plt.hist(char_word_ratio, bins\u003d50, edgecolor\u003d\u0027black\u0027)\n",
        "plt.title(\u0027Distribution of Character-to-Word Ratios\u0027)\n",
        "plt.xlabel(\u0027Characters per Word\u0027)\n",
        "plt.ylabel(\u0027Frequency\u0027)\n",
        "plt.show()\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comments\n",
        "Tweets with excessive punctuations are often containing dots or a URL. Therefore, it is not necessary to remove them from the dataset. "
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Replace \u0026quot; with \" in text column\n",
        "df[\u0027text\u0027] \u003d df[\u0027text\u0027].str.replace(\u0027\u0026quot;\u0027, \u0027\"\u0027)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE_ASSISTANT_MAGIC_CELL\n",
        "# %load_ext ai_code_assistant\n",
        "\n",
        "# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n",
        "import dataiku\n",
        "import pandas as pd, numpy as np\n",
        "from dataiku import pandasutils as pdu\n",
        "import os\n",
        "import tempfile\n",
        "\n",
        "# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n",
        "project \u003d dataiku.api_client().get_default_project()\n",
        "client \u003d dataiku.api_client()\n",
        "\n",
        "# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n",
        "# Retrieve Kaggle username and api key\n",
        "auth_info \u003d client.get_auth_info(with_secrets\u003dTrue)\n",
        "secret_value \u003d None\n",
        "for secret in auth_info[\"secrets\"]:\n",
        "    if secret[\"key\"] \u003d\u003d \"KAGGLE_API_KEY\":\n",
        "        os.environ[\"KAGGLE_KEY\"] \u003d secret[\"value\"]\n",
        "        \n",
        "    elif secret[\"key\"] \u003d\u003d \"KAGGLE_USERNAME\":\n",
        "        os.environ[\"KAGGLE_USERNAME\"] \u003d secret[\"value\"]\n",
        "        \n",
        "from kaggle import api # import the already authenticated API client\n",
        "\n",
        "# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n",
        "# Retrieve the folder id where the dataset will be stored\n",
        "folder_id \u003d next((folder[\"id\"] for folder in project.list_managed_folders() if folder[\"name\"]\u003d\u003d\"data\"), None)\n",
        "if folder_id is None:\n",
        "    print(\"Folder \u0027data\u0027 not found!\")\n",
        "\n",
        "# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n",
        "import chardet\n",
        "folder \u003d dataiku.Folder(folder_id)\n",
        "dataset_slug \u003d \"kazanova/sentiment140\"\n",
        "with tempfile.TemporaryDirectory() as tmpdirname:\n",
        "    \n",
        "    api.dataset_download_files(dataset_slug, path\u003dtmpdirname, unzip\u003dTrue)\n",
        "\n",
        "    for file in os.listdir(tmpdirname):\n",
        "        local_file \u003d os.path.join(tmpdirname, file)\n",
        "        folder.upload_file(file, local_file)\n",
        "        imported_tweets_df \u003d pd.read_csv(local_file, encoding\u003d\"latin-1\", \n",
        "                                       names\u003d[\u0027target\u0027, \u0027id\u0027, \u0027date\u0027, \u0027flag\u0027, \u0027user\u0027, \u0027text\u0027])\n",
        "\n",
        "# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n",
        "# Write recipe outputs\n",
        "imported_tweets \u003d dataiku.Dataset(\"imported_tweets\")\n",
        "imported_tweets.write_with_schema(imported_tweets_df)\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Textual data visual exploration"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Word Cloud Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# Combine all tweets into one text\n",
        "all_text \u003d \u0027 \u0027.join(df[\u0027text\u0027].astype(str))\n",
        "\n",
        "# Clean text - remove URLs, mentions, special chars\n",
        "all_text \u003d re.sub(r\u0027http\\S+|@\\S+|[^\\w\\s]\u0027, \u0027\u0027, all_text.lower())\n",
        "\n",
        "# Create word cloud\n",
        "wordcloud \u003d WordCloud(width\u003d1200, height\u003d600, \n",
        "                     background_color\u003d\u0027white\u0027,\n",
        "                     max_words\u003d100).generate(all_text)\n",
        "\n",
        "# Display the word cloud\n",
        "plt.figure(figsize\u003d(12, 6))\n",
        "plt.imshow(wordcloud, interpolation\u003d\u0027bilinear\u0027)\n",
        "plt.axis(\u0027off\u0027)\n",
        "plt.title(\u0027Most Frequent Words in Tweets\u0027)\n",
        "plt.show()\n"
      ],
      "outputs": []
    }
  ]
}