{
  "metadata": {
    "associatedRecipe": "recipe_from_notebook_quality_analysis_textual_data",
    "createdOn": 1742205346782,
    "creator": "anne-soline.guilbert-ly@dataiku.com",
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python (env edf_sentiment_analysis)",
      "language": "python",
      "name": "py-dku-venv-edf_sentiment_analysis"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    },
    "modifiedBy": "anne-soline.guilbert-ly@dataiku.com",
    "tags": [
      "recipe-editor"
    ],
    "dkuGit": {
      "lastInteraction": 1742320941936,
      "gitReference": {
        "remote": "git@github.com:annesoline/sentiment-analysis.git",
        "checkout": "refs/heads/main",
        "remotePath": "quality_analysis_textual_data.ipynb",
        "lastHash": "70fb655a8e3cf502108975cc7a3b47ef2e591384",
        "lastTimestamp": 1742320920000,
        "isDirty": false
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import string\n",
        "pd.set_option(\u0027display.max_colwidth\u0027, None)\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Load tweets dataset"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df \u003d pd.read_csv(\u0027tweets.csv\u0027, encoding\u003d\u0027latin-1\u0027, names\u003d[\u0027target\u0027, \u0027id\u0027, \u0027date\u0027, \u0027flag\u0027, \u0027user\u0027, \u0027text\u0027, \u0027tweet_length_chars\u0027, \u0027tweet_length_words\u0027, \u0027mention_only\u0027, \u0027unreadable\u0027, \u0027too_many_numbers\u0027])"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df.head()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Textual data quality analysis\n",
        "## 2.1. Basic information"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df.info()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Values taken by the column flag: {df[\u0027flag\u0027].unique()[0]}\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"There are {df[\u0027user\u0027].nunique()} different users.\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check for empty tweets\n",
        "empty_tweets \u003d len(df[df[\u0027text\u0027].str.len() \u003d\u003d 0])\n",
        "print(f\"\\nNumber of empty tweets: {empty_tweets}\")\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check for duplicate tweets\n",
        "duplicate_tweets \u003d df.duplicated(subset\u003d\u0027text\u0027).sum()\n",
        "print(f\"Number of duplicate tweets: {duplicate_tweets}\")\n",
        "# Display some duplicated tweets\n",
        "duplicated_tweets_df \u003d df[df.duplicated(subset\u003d\u0027text\u0027, keep\u003dFalse)]\n",
        "print(\"\\nSample of duplicated tweets:\")\n",
        "print(duplicated_tweets_df[[\u0027text\u0027]].head())\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Function to preprocess text by converting to lowercase and removing punctuation\n",
        "def preprocess_text(text):\n",
        "    return text.lower().translate(str.maketrans(\u0027\u0027, \u0027\u0027, string.punctuation))\n",
        "\n",
        "# Check for duplicate tweets (case insensitive and ignoring punctuation)\n",
        "duplicate_tweets_case_insensitive_no_punct \u003d df[\u0027text\u0027].apply(preprocess_text).duplicated().sum()\n",
        "print(f\"Number of duplicate tweets (case insensitive and ignoring punctuation): {duplicate_tweets_case_insensitive_no_punct}\")\n",
        "\n",
        "# Create a new column \u0027is_duplicated\u0027 to tag duplicated tweets, except the first one\n",
        "df[\u0027is_duplicated\u0027] \u003d df[\u0027text\u0027].apply(preprocess_text).duplicated(keep\u003d\u0027first\u0027).astype(int)\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2. Tweet length"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a new dataframe with the original columns plus the new length columns\n",
        "df[\u0027tweet_length_chars\u0027] \u003d df[\u0027text\u0027].str.len()\n",
        "df[\u0027tweet_length_words\u0027] \u003d df[\u0027text\u0027].str.split().apply(len)\n",
        "df.head()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Tweet column analysis\n",
        "print(\"\\nTweet length statistics:\")\n",
        "print(df[\u0027tweet_length_chars\u0027].describe())\n",
        "\n",
        "# Plot distribution of tweet lengths\n",
        "plt.figure(figsize\u003d(12,6))\n",
        "plt.hist(df[\u0027tweet_length_chars\u0027], bins\u003d50, edgecolor\u003d\u0027black\u0027)\n",
        "plt.title(\u0027Distribution of Tweet Lengths\u0027)\n",
        "plt.xlabel(\u0027Number of Characters\u0027)\n",
        "plt.ylabel(\u0027Frequency\u0027)\n",
        "plt.show()\n",
        "\n",
        "# Most common tweet lengths in words\n",
        "print(\"\\nMost common tweet lengths (in words):\")\n",
        "print(df[\u0027tweet_length_words\u0027].value_counts().head())"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3. Very short tweets"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check for very short tweets that might be low quality\n",
        "# Create a new DataFrame with only very short tweets\n",
        "df_very_short_tweets \u003d df[df[\u0027tweet_length_chars\u0027] \u003c 10]\n",
        "\n",
        "# Count and display the number of very short tweets\n",
        "very_short_tweets_count \u003d df_very_short_tweets.shape[0]\n",
        "print(f\"\\nVery short tweets (\u003c10 chars): {very_short_tweets_count} ({(very_short_tweets_count/len(df)*100):.2f}%)\")\n",
        "\n",
        "# Display examples of very short tweets\n",
        "print(\"\\nExamples of very short tweets:\")\n",
        "print(df_very_short_tweets[\u0027text\u0027].head(10))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comments\n",
        "As shown in the example above, very short tweets can still be used for sentiment analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4. Tweet specificities (characters, URL, and mentions)"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "scrolled": false
      },
      "source": [
        "# Unique characters analysis\n",
        "all_chars \u003d \u0027\u0027.join(df[\u0027text\u0027].values)\n",
        "unique_chars \u003d set(all_chars)\n",
        "print(f\"\\nNumber of unique characters used: {len(unique_chars)}\")\n",
        "print(f\"Unique characters used: {\u0027\u0027.join(sorted(unique_chars))}\")\n",
        "\n",
        "# Create a new column \u0027repetitive_chars\u0027 with 1 if there are repetitive characters and 0 otherwise\n",
        "df[\u0027repetitive_chars\u0027] \u003d df[\u0027text\u0027].str.contains(r\u0027(.)\\1{4,}\u0027).astype(int)\n",
        "\n",
        "# Count tweets with repetitive characters\n",
        "repetitive_chars \u003d df[\u0027repetitive_chars\u0027].sum()\n",
        "print(f\"\\nTweets with repetitive characters: {repetitive_chars} ({(repetitive_chars/len(df)*100):.2f}%)\")\n",
        "\n",
        "# Display examples of tweets with repetitive characters\n",
        "print(\"\\nExamples of tweets with repetitive characters:\")\n",
        "print(df[df[\u0027repetitive_chars\u0027] \u003d\u003d 1][\u0027text\u0027].head(10))\n",
        "\n",
        "# URL and mention analysis\n",
        "tweets_with_urls \u003d len(df[df[\u0027text\u0027].str.contains(\u0027http|www\u0027, regex\u003dTrue)])\n",
        "tweets_with_mentions \u003d len(df[df[\u0027text\u0027].str.contains(\u0027@\u0027)])\n",
        "\n",
        "# Print examples of tweets with URLs\n",
        "print(\"\\nExample tweets containing URLs:\")\n",
        "df_with_urls \u003d df[df[\u0027text\u0027].str.contains(\u0027http|www\u0027, regex\u003dTrue)].copy()\n",
        "print(df_with_urls[\u0027text\u0027].head())\n",
        "\n",
        "# Print examples of tweets with mentions\n",
        "print(\"\\nExample tweets containing @mentions:\")\n",
        "print(df[df[\u0027text\u0027].str.contains(\u0027@\u0027)][\u0027text\u0027].head())\n",
        "\n",
        "print(f\"\\nTweets containing URLs: {tweets_with_urls} ({tweets_with_urls/len(df)*100:.2f}%)\")\n",
        "print(f\"Tweets containing @mentions: {tweets_with_mentions} ({tweets_with_mentions/len(df)*100:.2f}%)\")\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comments\n",
        "Tweets with repetitive characters often contain repeated dots or letters. For the latter, we can easily eliminate the repetition in the letters, helping the model better understand the words.\n",
        "\n",
        "It will be useful to remove all the URL from the text to ease the detection of sentiment in the text.\n",
        "\n",
        "Tweets containing only a mention in the text are tagged, allowing them to be removed from the dataset later, as they provide no relevant information for sentiment analysis."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create new dataframe with mention_only column\n",
        "df[\u0027mention_only\u0027] \u003d df[\u0027text\u0027].str.match(r\u0027^\\s*@\\w+\\s*$\u0027).astype(int)\n",
        "\n",
        "# Print summary\n",
        "print(f\"\\nTweets that are only mentions: {df[\u0027mention_only\u0027].sum()} ({df[\u0027mention_only\u0027].sum()/len(df)*100:.2f}%)\")\n",
        "print(\"\\nExample tweets that are only mentions:\")\n",
        "print(df[df[\u0027mention_only\u0027] \u003d\u003d 1][\u0027text\u0027].head())\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.5. Tweets with special characters and unreadable tweets"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import string\n",
        "\n",
        "special_chars \u003d [c for c in unique_chars \n",
        "                if c not in string.ascii_letters \n",
        "                and c not in string.digits\n",
        "                and c not in string.punctuation\n",
        "                and not c.isalpha()\n",
        "                and c not in [\u0027¸\u0027, \u0027·\u0027, \u0027 \u0027, \u0027´\u0027, \u0027»\u0027, \u0027«\u0027]]  # Excludes accented letters, euro symbol, and specific characters\n",
        "print(\u0027Special characters:\u0027, (sorted(special_chars)))"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create new dataframe with only unreadable rows\n",
        "# Create a new column in df with a tag 1/0 if the text is unreadable\n",
        "df[\u0027unreadable\u0027] \u003d df[\u0027text\u0027].apply(lambda x: int(sum(1 for c in x if c in special_chars) \u003e 5))\n",
        "\n",
        "# Create a new dataframe with only unreadable rows\n",
        "df_unreadable \u003d df[df[\u0027unreadable\u0027] \u003d\u003d 1].copy()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"\\nTweets with more than 5 special characters:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"\\nNumber of unreadable tweets: {len(df_unreadable)}\")\n",
        "print(\"\\nExample unreadable tweets:\")\n",
        "print(df_unreadable[[\u0027text\u0027]].head(10))\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check for tweets with high percentage of numbers\n",
        "df_high_percentage_numbers \u003d df.copy()\n",
        "number_ratio \u003d df_high_percentage_numbers[\u0027text\u0027].str.count(r\u0027[0-9]\u0027) / df_high_percentage_numbers[\u0027tweet_length_chars\u0027]\n",
        "df_high_percentage_numbers[\u0027too_many_numbers\u0027] \u003d (number_ratio \u003e 0.3).astype(int)\n",
        "df[\u0027too_many_numbers\u0027] \u003d df_high_percentage_numbers[\u0027too_many_numbers\u0027]\n",
        "high_numbers \u003d df_high_percentage_numbers[\u0027too_many_numbers\u0027].sum()\n",
        "print(f\"\\nTweets with high number ratio (\u003e30%): {high_numbers} ({(high_numbers/len(df_high_percentage_numbers)*100):.2f}%)\")\n",
        "print(\"\\nExamples of tweets with many numbers:\")\n",
        "print(df_high_percentage_numbers[df_high_percentage_numbers[\u0027too_many_numbers\u0027] \u003d\u003d 1][\u0027text\u0027].head(10))\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comments\n",
        "In this section, we reviewed tweets containing excessive special characters that render them unreadable, and created a column to tag these tweets so they can be removed from the dataset later. We did the same for the tweets with too many numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.6. Word/character ratio"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Analyze word/character ratio (very low ratio might indicate spam or low quality)\n",
        "char_word_ratio \u003d df[\u0027tweet_length_chars\u0027] / df[\u0027tweet_length_words\u0027]\n",
        "suspicious_ratio \u003d df[char_word_ratio \u003e char_word_ratio.mean() + 2*char_word_ratio.std()].shape[0]\n",
        "print(f\"\\nTweets with suspicious character-to-word ratio: {suspicious_ratio} ({(suspicious_ratio/len(df)*100):.2f}%)\")\n",
        "print(\"\\nExamples of tweets with suspicious character-to-word ratio:\")\n",
        "print(df[char_word_ratio \u003e char_word_ratio.mean() + 2*char_word_ratio.std()][\u0027text\u0027].head(10))\n",
        "\n",
        "# Distribution of character-to-word ratios\n",
        "plt.figure(figsize\u003d(12, 6))\n",
        "plt.hist(char_word_ratio, bins\u003d50, edgecolor\u003d\u0027black\u0027)\n",
        "plt.title(\u0027Distribution of Character-to-Word Ratios\u0027)\n",
        "plt.xlabel(\u0027Characters per Word\u0027)\n",
        "plt.ylabel(\u0027Frequency\u0027)\n",
        "plt.show()\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comments\n",
        "Tweets with excessive punctuations are often containing dots or a URL. Therefore, it is not necessary to remove them from the dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.7. Language detection"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from ftlangdetect import detect\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        return detect(text)[\u0027lang\u0027]\n",
        "    except Exception as e:\n",
        "        return \u0027unknown\u0027\n",
        "\n",
        "# Create a new column with the detected language\n",
        "df[\u0027language\u0027] \u003d df[\u0027text\u0027].apply(detect_language)\n",
        "\n",
        "# Display the count of tweets per detected language\n",
        "language_counts \u003d df[\u0027language\u0027].value_counts()\n",
        "print(\"\\nLanguage distribution in tweets:\")\n",
        "print(language_counts)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df.head()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": []
    }
  ]
}