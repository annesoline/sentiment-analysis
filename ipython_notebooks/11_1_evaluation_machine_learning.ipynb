{
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.9.20",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "py-dku-venv-edf_sentiment_analysis",
      "display_name": "Python (env edf_sentiment_analysis)",
      "language": "python"
    },
    "hide_input": false,
    "customFields": {},
    "createdOn": 1743093158027,
    "creator": "anne-soline.guilbert-ly@dataiku.com",
    "tags": [
      "recipe-editor"
    ],
    "modifiedBy": "anne-soline.guilbert-ly@dataiku.com",
    "associatedRecipe": "11_1_evaluation_machine_learning"
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluate"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n\ndata_path \u003d \u0027data/tweets_encryption_test.csv\u0027\ndata \u003d pd.read_csv(data_path)\n\n# Convert the data to a DataFrame\neval_df \u003d pd.DataFrame(data)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pickle\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nimport os\nimport tempfile\n\n# Convert the data to a DataFrame\n# tweets_eval \u003d dataiku.Dataset(\"tweets_eval\")\n# eval_df \u003d tweets_eval.get_dataframe()\n\n# Get latest model in \n# folder \u003d dataiku.Folder(\"20Z7bzGW\")\nPICKLE_MODELS_PATH \u003d \u0027pickle_models/\u0027\n\n# Get the list of all files in the directory\nfiles \u003d os.listdir(PICKLE_MODELS_PATH)\n\n# Filter out non-pickle files and sort by modification time\npickle_files \u003d [f for f in files if f.endswith(\u0027.pkl\u0027)]\npickle_files.sort(key\u003dlambda f: os.path.getmtime(os.path.join(PICKLE_MODELS_PATH, f)))\n\n# Get the latest model file\nlatest_model_file \u003d pickle_files[-1] if pickle_files else None\n\n# Construct the full path to the latest model\nmodel_path \u003d os.path.join(PICKLE_MODELS_PATH, latest_model_file) if latest_model_file else None\n\n# Load the latest model\nwith tempfile.TemporaryDirectory() as temp_directory_name:\n\n        # local_file_path \u003d temp_directory_name + \"/\" + model_name\n        local_file_path \u003d model_path\n\n        # Copy file from remote to local\n        with folder.get_download_stream(model_path) as f_remote, open(local_file_path,\u0027wb\u0027) as f_local:\n            shutil.copyfileobj(f_remote, f_local)\n\n        # Load the pipeline\n        with open(local_file_path, \u0027rb\u0027) as file:\n            pipeline \u003d pickle.load(file)\n\nmodel_name \u003d model_path.split(\"/\")[-1].split(\".pkl\")[0]\n\n\nX_eval, y_eval, _ \u003d preprocess_data(eval_df, tfidf)\n\neval_prediction_results_df \u003d eval_df.copy()\n\neval_prediction_results_df[\u0027predicted\u0027] \u003d lr_model.predict(X_eval)\neval_prediction_results_df[\u0027probability\u0027] \u003d lr_model.predict_proba(X_eval)[:, 1]\neval_prediction_results_df[\u0027correct_prediction\u0027] \u003d results_df[\u0027label\u0027] \u003d\u003d results_df[\u0027predicted\u0027]\n\n# Calculate evaluation metrics\naccuracy \u003d accuracy_score(y_eval, results_df[\u0027predicted\u0027])\nprecision \u003d precision_score(y_eval, results_df[\u0027predicted\u0027], average\u003d\u0027weighted\u0027)\nrecall \u003d recall_score(y_eval, results_df[\u0027predicted\u0027], average\u003d\u0027weighted\u0027)\nf1 \u003d f1_score(y_eval, results_df[\u0027predicted\u0027], average\u003d\u0027weighted\u0027)\nroc_auc \u003d roc_auc_score(y_eval, lr_model.predict_proba(X_eval), multi_class\u003d\u0027ovr\u0027, average\u003d\u0027weighted\u0027)\n\n# Print the evaluation metrics\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"Precision: {precision}\")\nprint(f\"Recall: {recall}\")\nprint(f\"F1 Score: {f1}\")\nprint(f\"ROC AUC: {roc_auc}\")\n\neval_metrics_df \u003d pd.DataFrame({\n    \u0027Mean Accuracy\u0027: [accuracy],\n    \u0027Mean Precision\u0027: [precision],\n    \u0027Mean Recall\u0027: [recall],\n    \u0027Mean F1 Score\u0027: [f1],\n    \u0027Mean ROC AUC\u0027: [roc_auc],\n    \u0027Model Name\u0027: [\u0027Logistic Regression\u0027],\n    \u0027Date and Time\u0027: [datetime.now().strftime(\u0027%Y-%m-%d %H:%M:%S\u0027)]\n})\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create output datasets"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Recipe outputs\neval_prediction_results \u003d dataiku.Dataset(\"eval_prediction_results\")\neval_prediction_results.write_with_schema(eval_prediction_results_df)\n\neval_metrics \u003d dataiku.Dataset(\"eval_metrics\")\neval_metrics.write_with_schema(eval_metrics_df)\n"
      ],
      "outputs": []
    }
  ]
}